---
title: DuckDB + dplyr (R)
subtitle: Use a familiar R frontend
execute:
  freeze: auto
  cache: true
---

## Load libraries

```{r dbplyr_libs}
#| cache: false

library(duckdb)
library(dplyr, warn.conflicts = FALSE)
library(tidyr, warn.conflicts = FALSE)
```

## Create a database connection

For the `d(b)plyr` workflow, the connection step is very similar to the pure SQL
approach. The only difference is that, after instantiating the database
connection, we need to register our parquet dataset as a table in our connection
via the `dplyr::tbl()` function. Note that we also assign it to an object (here:
`nyc`) that can be referenced from R.

```{r dbplyr_con}
#| cache: false

## Instantiate the in-memory DuckDB connection 
con = dbConnect(duckdb(), shutdown = TRUE)

## Register our parquet dataset as table in our connection (and that assign it
## to an object that R understands)
# nyc = tbl(con, "nyc-taxi/**/*.parquet") # works, but safer to use the read_parquet func)
nyc = tbl(con, "read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)")
```

## First example

This next command runs instantly because all computation is deferred (i.e.,
lazy eval). In other words, it is just a query object.

```{r dbplyr_q1}
q1 = nyc |>
  summarize(
    mean_tip = mean(tip_amount),
    .by = passenger_count
  )
```

:::{.callout-tip}
## `.by` versus `group_by` 
In case you weren't aware: `summarize(..., .by = x)` is a shorthand (and
non-persistent) version of `group_by(x) |> summarize(...)`. More details
[here](https://www.tidyverse.org/blog/2023/02/dplyr-1-1-0-per-operation-grouping/).
:::

We can see what DuckDB's query tree looks like by asking it to explain
the plan

```{r dbplyr_q1_explain}
explain(q1)
```

Similarly, to show the SQL translation that will be implemented on the backend,
using `show_query`.

```{r dbplyr_q1_show_query}
show_query(q1)
```

Note that printing the query object actually does enforce some computation.
OTOH it's still just a preview of the data (we haven't pulled everything into
R's memory).

```{r dbplyr_q1_print}
q1
```

To actually pull all of the result data into R, we must call `collect()`
on the query object
```{r dbplyr_q1_collect}
tic = Sys.time()
dat1 = collect(q1)
toc = Sys.time()

dat1
toc - tic
```

```{r dbplyr_dat_time}
#| include: false
dat1_time = sprintf("%.2f", toc - tic)
```

## Aggregation

Here's our earlier filtering example with multiple grouping + aggregation
variables...

```{r dbplyr_q2}
q2 = nyc |>
  filter(month <= 3) |>
  summarize(
    across(c(tip_amount, fare_amount), \(x) mean(x)),
    .by = c(month, passenger_count)
  )
q2
```

Aside: note the optimised query includes hash groupings and projection
(basically: fancy column subsetting, which is a suprisingly effective strategy
in query optimization)



```{r dbplyr_q2_explain}
explain(q2)
```

And our high-dimensional aggregation example. We'll create a query for this
first, since I'll reuse it shortly again

```{r dbplyr_q3}
q3 = nyc |>
  group_by(passenger_count, trip_distance) |>
  summarize(
    across(c(tip_amount, fare_amount), \(x) mean(x)),
  ) 
collect(q3)
```


We can also do multiple things at once using familiar dplyr syntax


```{r}
q4 = nyc |>
summarise(across(c(tip_amount, fare_amount), list(Median = \(x) median(x),
                                                  Mean = \(x) mean(x),
                                                  Max = \(x) max(x)),
                                                  .names = '{.col}_{.fn}')) 

collect(q4)                                                 


```




## Pivot (reshape)

```{r dbplyr_q3_pivot}
# library(tidyr) ## already loaded

q3 |>
  pivot_longer(tip_amount:fare_amount) |>
  collect()
```

## Joins (merges)

```{r dbplyr_join0}
mean_tips  = nyc |> summarise(mean_tips = mean(tip_amount), .by = month)
mean_fares = nyc |> summarise(mean_fares = mean(fare_amount), .by = month)
```

Again, these commands complete instantly because all computation has been
deferred until absolutely necessary (i.e.,. lazy eval).

```{r dbplyr_join1}
left_join(
  mean_fares,
  mean_tips
  ) |>
  collect()
```

## Windowing

If you recall from the native SQL API, we sampled 1 percent of the data before
creating decile bins to reduce the computation burden of sorting the entire
table. Unfortunately, this approach doesn't work as well for the **dplyr**
frontend because the underlying SQL translation
[uses](https://dbplyr.tidyverse.org/reference/dbplyr-slice.html) a generic
sampling approach (rather than DuckDB's optimised `USING SAMPLE` statement.)

## Close connection

```{r dbplyr_con_close}
#| cache: false

dbDisconnect(con)
```

## Appendix: Related interfaces

### arrow+duckdb

```{r arrow_libs}
#| message: false
#| warning: false
library(arrow)
library(duckdb) ## Already loaded
library(dplyr)  ## Already loaded
library(tidyr)  ## Already loaded
```

When going through the **arrow** intermediary, we don't need to establish a
database with `DBI::dbConnect` like we did above. Instead, we can create a link
(pointers) to the dataset on disk directly via the `arrow::open_dataset()`
convience function. Here I'll assign it to a new R object called `nyc2`.

```{r nyc2}
nyc2 = open_dataset("nyc-taxi")
```

:::{.callout-tip}
## open_dataset() versus read_parquet()
(For individual parquet files, we could just read then via
`arrow::read_parquet()`, perhaps efficiently subsetting columns at the same
time. But I find the `open_dataset` is generally what I'm looking for.)
:::

Note that printing our `nyc2` dataset to the R console will just display the
data schema. This is a cheap and convenient way to quickly interrogate the basic
structure of your data, including column types, etc.

```{r nyc2_print}
nyc2
```

The key step for this "arrow + duckdb" **dplyr** workflow is to pass our arrow
dataset to DuckDB via the `to_duckdb()` function.

```{r nyc2_to_duckdb}
to_duckdb(nyc2)
```

Note that this transfer from Arrow to DuckDB is very quick (and memory cheap)
because it is a zero copy. We are just passing around pointers instead of
actually moving any data. See
[this blog post](https://duckdb.org/2021/12/03/duck-arrow.html)
for more details, but the high-level take away is that we are benefitting from
the tightly integrated architectures of these two libraries.^["Similar" might be
a better description than "integrated", since DuckdB does not use the Arrow
memory model. But they are both columnar-orientated (among other things) and so
the end result is pretty seamless integration.]

At this, point all of the regular **dplyr** workflow logic from above should
carry over. Just remember to first pass the arrow dataset via the `to_duckdb()`
funciton. For example, here's our initial aggregation query again:

```{r nyc2_q1}
nyc2 |>
  to_duckdb() |> ## <= key step
  summarise(
    mean_tip = mean(tip_amount),
    .by = passenger_count
  ) |>
  collect()
```

:::{.callout-note}
## Arrow's native acero engine

Some of you may be used to performing computation with the **arrow** package
without going through DuckDB. What's happening here is that arrow provides its
own computation engine called "acero". This Arrow-native engine is actually
pretty performant... albeit not a fast as DuckDB, nor as feature rich. So I
personally recommend always passing to DuckDB if you can. Still, if you're
curious then you can test yourself by re-trying the code chunk, but commenting
out the `to_duckdb()` line. For more details, see
[here](https://youtu.be/LvTX1ZAZy6M?si=7gZYG03ojtAtPGfe).
:::

### duckplyr

The new kid on the block is **duckplyr**
([announcement](https://duckdb.org/2024/04/02/duckplyr.html) /
[homepage](https://duckdblabs.github.io/duckplyr/)).
Without going into too much depth, the promise of **duckplyr** is that it can
provide a "fully native" dplyr experience that is _directly_ coupled to DuckDB's
query engine. So, for example, it won't have to rely on **DBI**'s generic' SQL
translations. Instead, the relevant **dplyr** "verbs" are being directly
translated to DuckDB's relational API to construct logical query plans. If
that's too much jargon, just know that it should involve less overhead, fewer
translation errors, and better optimization. Moreover, a goal of **duckplyr** is
for it to be a drop-in replace for **dplyr** _in general_. In other words, you
could just swap out `library(dplyr)` for `library(duckplyr)` and all of your
data wrangling operations will come backed by the power of DuckDB. This includes
for working on "regular" R data frames in memory.

All of this is exciting and I would urge you stay tuned. Right now, **duckplyr**
is still marked as experimental and has a few rough edges. But the basics are
there. For example:

```{r duckplyr_q1}
library(duckplyr, warn.conflicts = FALSE)

duckplyr_df_from_parquet("nyc-taxi/**/*.parquet") |>
  summarise(
    mean_tip = mean(tip_amount),
    .by = passenger_count
  )
```