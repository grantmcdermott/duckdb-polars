[
  {
    "objectID": "polars-rpy.html",
    "href": "polars-rpy.html",
    "title": "Polars from Python and R",
    "section": "",
    "text": "PythonR\n\n\n\nimport polars as pl\nimport time\nimport matplotlib\n\n\n\n\nlibrary(polars)\n\nWarning: package 'polars' was built under R version 4.4.1",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#load-libraries",
    "href": "polars-rpy.html#load-libraries",
    "title": "Polars from Python and R",
    "section": "",
    "text": "PythonR\n\n\n\nimport polars as pl\nimport time\nimport matplotlib\n\n\n\n\nlibrary(polars)\n\nWarning: package 'polars' was built under R version 4.4.1",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#scan-data",
    "href": "polars-rpy.html#scan-data",
    "title": "Polars from Python and R",
    "section": "Scan data",
    "text": "Scan data\n\nPythonR\n\n\n\nnyc = pl.scan_parquet(\"nyc-taxi/**/*.parquet\", hive_partitioning=True)\nnyc\n\nNAIVE QUERY PLANrun LazyFrame.show_graph() to see the optimized version\n\n\n\n\n\npolars_query\n\n\n\np1\n\nParquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other files]\nπ */24;\n\n\n\n\n\n\n\n\nnyc = pl$scan_parquet(\"nyc-taxi/**/*.parquet\", hive_partitioning=TRUE)\nnyc\n\npolars LazyFrame\n $explain(): Show the optimized query plan.\n\nNaive plan:\nParquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other files]\nPROJECT */24 COLUMNS",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#first-example",
    "href": "polars-rpy.html#first-example",
    "title": "Polars from Python and R",
    "section": "First example",
    "text": "First example\nPolars operations are registered as queries until they are collected.\n\nPythonR\n\n\n\nq1 = (\n    nyc\n    .group_by([\"passenger_count\"])\n    .agg([\n            pl.mean(\"tip_amount\")#.alias(\"mean_tip\") ## alias is optional\n        ])\n    .sort(\"passenger_count\")\n)\nq1\n\nNAIVE QUERY PLANrun LazyFrame.show_graph() to see the optimized version\n\n\n\n\n\npolars_query\n\n\n\np1\n\nSORT BY [col(\"passenger_count\")]\n\n\n\np2\n\nAGG [col(\"tip_amount\").mean()]\nBY\n[col(\"passenger_count\")]\n\n\n\np1--p2\n\n\n\n\np3\n\nParquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other files]\nπ */22;\n\n\n\np2--p3\n\n\n\n\n\n\n\n\n\nq1 = (\n    nyc\n    $group_by(\"passenger_count\")\n    $agg(\n        pl$mean(\"tip_amount\")#$alias(\"mean_tip\") ## alias is optional\n    )\n    $sort(\"passenger_count\")\n)\nq1 \n\npolars LazyFrame\n $explain(): Show the optimized query plan.\n\nNaive plan:\nSORT BY [col(\"passenger_count\")]\n  AGGREGATE\n    [col(\"tip_amount\").mean()] BY [col(\"passenger_count\")] FROM\n    Parquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other files]\n    PROJECT */24 COLUMNS\n\n\n\n\n\n\n\n\nR-polars multiline syntax\n\n\n\nPolars-style x$method1()$method2()... chaining may seem a little odd to R users, especially for multiline queries. Here I have adopted the same general styling as Python: By enclosing the full query in parentheses (), we can start each $method() on a new line. If this isn’t to your fancy, you could also rewrite these multiline queries as follows:\nnyc$group_by(\n    \"passenger_count\"\n)$agg(\n    pl$mean(\"tip_amount\")\n)$sort(\"passenger_count\")\n\n\n\n\n\n(Note: this is the naive query plan, not the optimized query that polars will actually implement for us. We’ll come back to this idea shortly.)\nCalling collect() enforces computation.\n\nPythonR\n\n\n\ntic = time.time()\ndat1 = q1.collect()\ntoc = time.time()\n\ndat1\n\n\nshape: (18, 2)\n\n\n\npassenger_count\ntip_amount\n\n\ni64\nf64\n\n\n\n\n0\n0.862099\n\n\n1\n1.151011\n\n\n2\n1.08158\n\n\n3\n0.962949\n\n\n4\n0.844519\n\n\n…\n…\n\n\n177\n1.0\n\n\n208\n0.0\n\n\n247\n2.3\n\n\n249\n0.0\n\n\n254\n0.0\n\n\n\n\n\n# print(f\"Time difference of {toc - tic} seconds\")\n\n\n\n\ntic = Sys.time()\ndat1 = q1$collect()\ntoc = Sys.time()\n\ndat1\n\n\nshape: (18, 2)\n\n\n\npassenger_count\ntip_amount\n\n\ni64\nf64\n\n\n\n\n0\n0.862099\n\n\n1\n1.151011\n\n\n2\n1.08158\n\n\n3\n0.962949\n\n\n4\n0.844519\n\n\n…\n…\n\n\n177\n1.0\n\n\n208\n0.0\n\n\n247\n2.3\n\n\n249\n0.0\n\n\n254\n0.0\n\n\n\n\n\ntoc - tic\n\nTime difference of 2.880659 secs",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#aggregation",
    "href": "polars-rpy.html#aggregation",
    "title": "Polars from Python and R",
    "section": "Aggregation",
    "text": "Aggregation\nSubsetting along partition dimensions allows for even more efficiency gains.\n\nPythonR\n\n\n\nq2 = (\n    nyc\n    .filter(pl.col(\"month\") &lt;= 3)\n    .group_by([\"month\", \"passenger_count\"])\n    .agg([pl.mean(\"tip_amount\").alias(\"mean_tip\")])\n    .sort(\"passenger_count\")\n)\n\n\n\n\nq2 = (\n    nyc\n    $filter(pl$col(\"month\") &lt;= 3)\n    $group_by(\"month\", \"passenger_count\")\n    $agg(pl$mean(\"tip_amount\")$alias(\"mean_tip\"))\n    $sort(\"passenger_count\")\n) \n\n\n\n\nLet’s take a look at the optimized query that Polars will implement for us.\n\nPythonR\n\n\n\n# q2             # naive\nq2.show_graph()  # optimized\n\n\n\n\n\n\n\n\n\n\n\n# q2              # naive\ncat(q2$explain()) # optimized\n\nSORT BY [col(\"passenger_count\")]\n  AGGREGATE\n    [col(\"tip_amount\").mean().alias(\"mean_tip\")] BY [col(\"month\"), col(\"passenger_count\")] FROM\n    Parquet SCAN [nyc-taxi/year=2012/month=1/part-0.parquet, ... 11 other files]\n    PROJECT 2/24 COLUMNS\n    SELECTION: [(col(\"month\").cast(Unknown(Float))) &lt;= (dyn float: 3.0)]\n\n\n\n\n\nNow, let’s run the query and collect the results.\n\nPythonR\n\n\n\ntic = time.time()\ndat2 = q2.collect()\ntoc = time.time()\n\ndat2\n\n\nshape: (29, 3)\n\n\n\nmonth\npassenger_count\nmean_tip\n\n\ni64\ni64\nf64\n\n\n\n\n3\n0\n0.877675\n\n\n1\n0\n0.841718\n\n\n2\n0\n0.876637\n\n\n3\n1\n1.089205\n\n\n2\n1\n1.06849\n\n\n…\n…\n…\n\n\n1\n9\n0.0\n\n\n2\n9\n0.0\n\n\n1\n65\n0.0\n\n\n3\n208\n0.0\n\n\n1\n208\n0.0\n\n\n\n\n\n# print(f\"Time difference of {toc - tic} seconds\")\n\n\n\n\ntic = Sys.time()\ndat2 = q2$collect()\ntoc = Sys.time()\n\ndat2\n\n\nshape: (29, 3)\n\n\n\nmonth\npassenger_count\nmean_tip\n\n\ni64\ni64\nf64\n\n\n\n\n3\n0\n0.877675\n\n\n1\n0\n0.841718\n\n\n2\n0\n0.876637\n\n\n3\n1\n1.089205\n\n\n1\n1\n1.036863\n\n\n…\n…\n…\n\n\n2\n9\n0.0\n\n\n1\n9\n0.0\n\n\n1\n65\n0.0\n\n\n1\n208\n0.0\n\n\n3\n208\n0.0\n\n\n\n\n\ntoc - tic\n\nTime difference of 3.721074 secs\n\n\n\n\n\nHigh-dimensional grouping example. This query provides an example where polars is noticeably slower than DuckDB.\n\nPythonR\n\n\n\nq3 = (\n    nyc\n    .group_by([\"passenger_count\", \"trip_distance\"])\n    .agg([\n        pl.mean(\"tip_amount\").alias(\"mean_tip\"),\n        pl.mean(\"fare_amount\").alias(\"mean_fare\"),\n        ])\n    .sort([\"passenger_count\", \"trip_distance\"])\n)\n\ntic = time.time()\ndat3 = q3.collect()\ntoc = time.time()\n\ndat3\n\n\nshape: (25_569, 4)\n\n\n\npassenger_count\ntrip_distance\nmean_tip\nmean_fare\n\n\ni64\nf64\nf64\nf64\n\n\n\n\n0\n0.0\n1.345135\n17.504564\n\n\n0\n0.01\n0.178571\n34.642857\n\n\n0\n0.02\n4.35\n61.05\n\n\n0\n0.03\n16.25\n74.0\n\n\n0\n0.04\n0.03\n46.5\n\n\n…\n…\n…\n…\n\n\n208\n5.1\n0.0\n12.5\n\n\n208\n6.6\n0.0\n17.7\n\n\n247\n3.31\n2.3\n11.5\n\n\n249\n1.69\n0.0\n8.5\n\n\n254\n1.02\n0.0\n6.0\n\n\n\n\n\n# print(f\"Time difference of {toc - tic} seconds\")\n\n\n\n\nq3 = (\n    nyc\n    $group_by(\"passenger_count\", \"trip_distance\")\n    $agg(\n        pl$mean(\"tip_amount\")$alias(\"mean_tip\"),\n        pl$mean(\"fare_amount\")$alias(\"mean_fare\")\n        )\n    $sort(\"passenger_count\", \"trip_distance\")\n)\n\ntic = Sys.time()\ndat3 = q3$collect()\ntoc = Sys.time()\n \ndat3\n\n\nshape: (25569, 4)\n\n\n\npassenger_count\ntrip_distance\nmean_tip\nmean_fare\n\n\ni64\nf64\nf64\nf64\n\n\n\n\n0\n0.0\n1.345135\n17.504564\n\n\n0\n0.01\n0.178571\n34.642857\n\n\n0\n0.02\n4.35\n61.05\n\n\n0\n0.03\n16.25\n74.0\n\n\n0\n0.04\n0.03\n46.5\n\n\n…\n…\n…\n…\n\n\n208\n5.1\n0.0\n12.5\n\n\n208\n6.6\n0.0\n17.7\n\n\n247\n3.31\n2.3\n11.5\n\n\n249\n1.69\n0.0\n8.5\n\n\n254\n1.02\n0.0\n6.0\n\n\n\n\n\ntoc - tic\n\nTime difference of 50.74207 secs\n\n\n\n\n\nAs an aside, if we didn’t care about column aliases (or sorting), then the previous query could be shortened to:\n\nPythonR\n\n\n(\n    nyc\n    .group_by([\"passenger_count\", \"trip_distance\"])\n    .agg(pl.col([\"tip_amount\", \"fare_amount\"]).mean())\n    .collect()\n)\n\n\n(\n    nyc\n    $group_by(\"passenger_count\", \"trip_distance\")\n    $agg(pl$col(\"tip_amount\", \"fare_amount\")$mean())\n    $collect()\n)",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#pivot-reshape",
    "href": "polars-rpy.html#pivot-reshape",
    "title": "Polars from Python and R",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\nIn polars, we have two distinct reshape methods:\n\npivot: =&gt; long to wide\nunpivot: =&gt; wide to long\n\nHere we’ll unpivot to go from wide to long and use the eager execution engine (i.e., on the dat3 DataFrame object that we’ve already computed) for expediency.\n\nPythonR\n\n\n\ndat3.unpivot(index = [\"passenger_count\", \"trip_distance\"])\n\n\nshape: (51_138, 4)\n\n\n\npassenger_count\ntrip_distance\nvariable\nvalue\n\n\ni64\nf64\nstr\nf64\n\n\n\n\n0\n0.0\n\"mean_tip\"\n1.345135\n\n\n0\n0.01\n\"mean_tip\"\n0.178571\n\n\n0\n0.02\n\"mean_tip\"\n4.35\n\n\n0\n0.03\n\"mean_tip\"\n16.25\n\n\n0\n0.04\n\"mean_tip\"\n0.03\n\n\n…\n…\n…\n…\n\n\n208\n5.1\n\"mean_fare\"\n12.5\n\n\n208\n6.6\n\"mean_fare\"\n17.7\n\n\n247\n3.31\n\"mean_fare\"\n11.5\n\n\n249\n1.69\n\"mean_fare\"\n8.5\n\n\n254\n1.02\n\"mean_fare\"\n6.0\n\n\n\n\n\n\n\n\n\ndat3$unpivot(index = c(\"passenger_count\", \"trip_distance\"))\n\n\nshape: (51138, 4)\n\n\n\npassenger_count\ntrip_distance\nvariable\nvalue\n\n\ni64\nf64\nstr\nf64\n\n\n\n\n0\n0.0\n\"mean_tip\"\n1.345135\n\n\n0\n0.01\n\"mean_tip\"\n0.178571\n\n\n0\n0.02\n\"mean_tip\"\n4.35\n\n\n0\n0.03\n\"mean_tip\"\n16.25\n\n\n0\n0.04\n\"mean_tip\"\n0.03\n\n\n…\n…\n…\n…\n\n\n208\n5.1\n\"mean_fare\"\n12.5\n\n\n208\n6.6\n\"mean_fare\"\n17.7\n\n\n247\n3.31\n\"mean_fare\"\n11.5\n\n\n249\n1.69\n\"mean_fare\"\n8.5\n\n\n254\n1.02\n\"mean_fare\"\n6.0",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#joins-merges",
    "href": "polars-rpy.html#joins-merges",
    "title": "Polars from Python and R",
    "section": "Joins (merges)",
    "text": "Joins (merges)\n\nPythonR\n\n\n\nmean_tips  = nyc.group_by(\"month\").agg(pl.col(\"tip_amount\").mean())\nmean_fares = nyc.group_by(\"month\").agg(pl.col(\"fare_amount\").mean())\n\n\n(\n    mean_tips\n    .join(\n        mean_fares,\n        on = \"month\",\n        how = \"left\" # default is inner join\n    )\n    .collect()\n)\n\n\nshape: (12, 3)\n\n\n\nmonth\ntip_amount\nfare_amount\n\n\ni64\nf64\nf64\n\n\n\n\n9\n1.254601\n12.391198\n\n\n3\n1.056353\n10.223107\n\n\n11\n1.250903\n12.270138\n\n\n6\n1.091082\n10.548651\n\n\n8\n1.079521\n10.49265\n\n\n…\n…\n…\n\n\n10\n1.281239\n12.501252\n\n\n5\n1.078014\n10.585157\n\n\n2\n1.036874\n9.94264\n\n\n4\n1.043167\n10.33549\n\n\n7\n1.059312\n10.379943\n\n\n\n\n\n\n\n\n\nmean_tips  = nyc$group_by(\"month\")$agg(pl$col(\"tip_amount\")$mean())\nmean_fares = nyc$group_by(\"month\")$agg(pl$col(\"fare_amount\")$mean())\n\n\n(\n    mean_tips\n    $join(\n        mean_fares,\n        on = \"month\",\n        how = \"left\"  # default is inner join\n    )\n    $collect()\n)\n\n\nshape: (12, 3)\n\n\n\nmonth\ntip_amount\nfare_amount\n\n\ni64\nf64\nf64\n\n\n\n\n7\n1.059312\n10.379943\n\n\n9\n1.254601\n12.391198\n\n\n8\n1.079521\n10.49265\n\n\n6\n1.091082\n10.548651\n\n\n12\n1.237651\n12.313953\n\n\n…\n…\n…\n\n\n3\n1.056353\n10.223107\n\n\n5\n1.078014\n10.585157\n\n\n2\n1.036874\n9.94264\n\n\n11\n1.250903\n12.270138\n\n\n1\n1.007817\n9.813488",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "polars-rpy.html#appendix-alternate-interfaces",
    "href": "polars-rpy.html#appendix-alternate-interfaces",
    "title": "Polars from Python and R",
    "section": "Appendix: Alternate interfaces",
    "text": "Appendix: Alternate interfaces\nThe native polars API is not the only way to interface with the underlying computation engine. Here are two alternate approaches that you may prefer, especially if you don’t want to learn a new syntax.\n\nIbis (Python)\nThe great advantage of Ibis (like dbplyr) is that it supports multiple backends through an identical frontend. So, all of our syntax logic and workflow from the Ibis+DuckDB section carry over to an equivalent Ibis+Polars workflow too. All you need to do is change the connection type. For example:\n\nimport ibis\nimport ibis.selectors as s\nfrom ibis import _\n\n##! This next line is the only thing that's changed !##\ncon = ibis.polars.connect()\n\ncon.register(\"nyc-taxi/**/*.parquet\", \"nyc\")\n\n&lt;string&gt;:2: FutureWarning: `Backend.register` is deprecated as of v9.1; use the explicit `read_*` method for the filetype you are trying to read, e.g., read_parquet, read_csv, etc.\n/Users/gmcd/Documents/Projects/duckdb-polars/.venv/lib/python3.12/site-packages/ibis/backends/polars/__init__.py:75: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n  schema = PolarsSchema.to_ibis(self._tables[name].schema)\nDatabaseTable: nyc\n  vendor_name           string\n  pickup_datetime       timestamp\n  dropoff_datetime      timestamp\n  passenger_count       int64\n  trip_distance         float64\n  pickup_longitude      float64\n  pickup_latitude       float64\n  rate_code             string\n  store_and_fwd         string\n  dropoff_longitude     float64\n  dropoff_latitude      float64\n  payment_type          string\n  fare_amount           float64\n  extra                 float64\n  mta_tax               float64\n  tip_amount            float64\n  tolls_amount          float64\n  total_amount          float64\n  improvement_surcharge float64\n  congestion_surcharge  float64\n  pickup_location_id    int64\n  dropoff_location_id   int64\n\nnyc = con.table(\"nyc\")\n\n(\n  nyc\n  .group_by([\"passenger_count\"])\n  .agg(mean_tip = _.tip_amount.mean())\n  .to_polars()\n)\n\nshape: (18, 2)\n┌─────────────────┬──────────┐\n│ passenger_count ┆ mean_tip │\n│ ---             ┆ ---      │\n│ i64             ┆ f64      │\n╞═════════════════╪══════════╡\n│ 6               ┆ 1.128365 │\n│ 9               ┆ 0.8068   │\n│ 3               ┆ 0.962949 │\n│ 0               ┆ 0.862099 │\n│ 66              ┆ 1.5      │\n│ …               ┆ …        │\n│ 8               ┆ 0.350769 │\n│ 5               ┆ 1.102732 │\n│ 65              ┆ 0.0      │\n│ 208             ┆ 0.0      │\n│ 247             ┆ 2.3      │\n└─────────────────┴──────────┘\n\n\n\n\ntidypolars (R)\nThe R package tidypolars (link) provides the “tidyverse” syntax while using polars as backend. The syntax and workflow should thus be immediately familar to R users.\nIt’s important to note that tidypolars is solely focused on the translation work. This means that you still need to load the main polars library alongside it for the actual computation, as well as dplyr (and potentially tidyr) for function generics.\n\nlibrary(polars) ## Already loaded\nlibrary(tidypolars)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)\n\nnyc = scan_parquet_polars(\"nyc-taxi/**/*.parquet\")\n\nnyc |&gt; \n    summarise(mean_tip = mean(tip_amount), .by = passenger_count) |&gt;\n    compute()\n\n\nshape: (18, 2)\n\n\n\npassenger_count\nmean_tip\n\n\ni64\nf64\n\n\n\n\n0\n0.862099\n\n\n9\n0.8068\n\n\n3\n0.962949\n\n\n6\n1.128365\n\n\n66\n1.5\n\n\n…\n…\n\n\n5\n1.102732\n\n\n8\n0.350769\n\n\n65\n0.0\n\n\n208\n0.0\n\n\n247\n2.3\n\n\n\n\n\n\nAside: Use collect() instead of compute() at the end if you would prefer to return a standard R data.frame instead of a Polars DataFrame.\nSee also polarssql (link) if you would like yet another “tidyverse”-esque alternative that works through DBI/d(b)plyr.",
    "crumbs": [
      "Polars from Python and R"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html",
    "href": "duckdb-dplyr.html",
    "title": "DuckDB + dplyr (R)",
    "section": "",
    "text": "library(duckdb)\n\nLoading required package: DBI\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#load-libraries",
    "href": "duckdb-dplyr.html#load-libraries",
    "title": "DuckDB + dplyr (R)",
    "section": "",
    "text": "library(duckdb)\n\nLoading required package: DBI\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#create-a-database-connection",
    "href": "duckdb-dplyr.html#create-a-database-connection",
    "title": "DuckDB + dplyr (R)",
    "section": "Create a database connection",
    "text": "Create a database connection\nFor the d(b)plyr workflow, the connection step is very similar to the pure SQL approach. The only difference is that, after instantiating the database connection, we need to register our parquet dataset as a table in our connection via the dplyr::tbl() function. Note that we also assign it to an object (here: nyc) that can be referenced from R.\n\n## Instantiate the in-memory DuckDB connection \ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n## Register our parquet dataset as table in our connection (and that assign it\n## to an object that R understands)\n# nyc = tbl(con, \"nyc-taxi/**/*.parquet\") # works, but safer to use the read_parquet func)\nnyc = tbl(con, \"read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)\")",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#first-example",
    "href": "duckdb-dplyr.html#first-example",
    "title": "DuckDB + dplyr (R)",
    "section": "First example",
    "text": "First example\nThis next command runs instantly because all computation is deferred (i.e., lazy eval). In other words, it is just a query object.\n\nq1 = nyc |&gt;\n  summarize(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n\n\n\n\n\n\n\n.by versus group_by\n\n\n\nIn case you weren’t aware: summarize(..., .by = x) is a shorthand (and non-persistent) version of group_by(x) |&gt; summarize(...). More details here.\n\n\nWe can see what DuckDB’s query tree looks like by asking it to explain the plan\n\nexplain(q1)\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n&lt;SQL&gt;\nSELECT passenger_count, AVG(tip_amount) AS mean_tip\nFROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\nGROUP BY passenger_count\n\n&lt;PLAN&gt;\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             #0            │\n│          avg(#1)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│      passenger_count      │\n│         tip_amount        │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│      passenger_count      │\n│         tip_amount        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│       EC: 179629584       │\n└───────────────────────────┘                             \n\n\nSimilarly, to show the SQL translation that will be implemented on the backend, using show_query.\n\nshow_query(q1)\n\n&lt;SQL&gt;\nSELECT passenger_count, AVG(tip_amount) AS mean_tip\nFROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\nGROUP BY passenger_count\n\n\nNote that printing the query object actually does enforce some computation. OTOH it’s still just a preview of the data (we haven’t pulled everything into R’s memory).\n\nq1\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB v0.10.1 [gmcd@Darwin 23.4.0:R 4.4.0/:memory:]\n   passenger_count mean_tip\n             &lt;dbl&gt;    &lt;dbl&gt;\n 1               5    1.10 \n 2              65    0    \n 3               9    0.807\n 4             177    1    \n 5               0    0.862\n 6             254    0    \n 7             249    0    \n 8               7    0.544\n 9               8    0.351\n10              66    1.5  \n# ℹ more rows\n\n\nTo actually pull all of the result data into R, we must call collect() on the query object\n\ntic = Sys.time()\ndat1 = collect(q1)\ntoc = Sys.time()\n\ndat1\n\n# A tibble: 18 × 2\n   passenger_count mean_tip\n             &lt;dbl&gt;    &lt;dbl&gt;\n 1               0    0.862\n 2             254    0    \n 3             249    0    \n 4               5    1.10 \n 5              65    0    \n 6               9    0.807\n 7             177    1    \n 8               2    1.08 \n 9             208    0    \n10              10    0    \n11               4    0.845\n12               1    1.15 \n13             247    2.3  \n14               3    0.963\n15               6    1.13 \n16               7    0.544\n17               8    0.351\n18              66    1.5  \n\ntoc - tic\n\nTime difference of 1.2924 secs",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#aggregation",
    "href": "duckdb-dplyr.html#aggregation",
    "title": "DuckDB + dplyr (R)",
    "section": "Aggregation",
    "text": "Aggregation\nHere’s our earlier filtering example with multiple grouping + aggregation variables…\n\nq2 = nyc |&gt;\n  filter(month &lt;= 3) |&gt;\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n    .by = c(month, passenger_count)\n  )\nq2\n\n# Source:   SQL [?? x 4]\n# Database: DuckDB v0.10.1 [gmcd@Darwin 23.4.0:R 4.4.0/:memory:]\n   month passenger_count tip_amount fare_amount\n   &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1     1               1      1.04         9.76\n 2     2               1      1.07         9.90\n 3     3               1      1.09        10.2 \n 4     1               8      0           21.3 \n 5     2               8      0.5          8.23\n 6     1               7      0            6.3 \n 7     1               3      0.875        9.87\n 8     1               6      1.02         9.86\n 9     2               3      0.895        9.98\n10     2               6      1.02         9.96\n# ℹ more rows\n\n\nAside: note the optimised query includes hash groupings and projection (basically: fancy column subsetting, which is a suprisingly effective strategy in query optimization)\n\nexplain(q2)\n\n&lt;SQL&gt;\nSELECT\n  \"month\",\n  passenger_count,\n  AVG(tip_amount) AS tip_amount,\n  AVG(fare_amount) AS fare_amount\nFROM (\n  SELECT q01.*\n  FROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\n  WHERE (\"month\" &lt;= 3.0)\n) q01\nGROUP BY \"month\", passenger_count\n\n&lt;PLAN&gt;\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             #0            │\n│             #1            │\n│          avg(#2)          │\n│          avg(#3)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│           month           │\n│      passenger_count      │\n│         tip_amount        │\n│        fare_amount        │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│      passenger_count      │\n│        fare_amount        │\n│         tip_amount        │\n│           month           │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│ File Filters: (month &lt;= 3)│\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│        EC: 44907396       │\n└───────────────────────────┘                             \n\n\nAnd our high-dimensional aggregation example. We’ll create a query for this first, since I’ll reuse it shortly again\n\nq3 = nyc |&gt;\n  group_by(passenger_count, trip_distance) |&gt;\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n  ) \ncollect(q3)\n\n# A tibble: 25,569 × 4\n# Groups:   passenger_count [18]\n   passenger_count trip_distance tip_amount fare_amount\n             &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1               1          0.7       0.493        5.04\n 2               2          0.8       0.462        5.44\n 3               1          0.8       0.535        5.36\n 4               1          1         0.616        6.01\n 5               1          0.3       0.334        4.11\n 6               1          4.8       1.70        16.0 \n 7               2          1         0.525        6.08\n 8               1          6.7       2.13        20.1 \n 9               1          0.4       0.361        4.17\n10               2          0.98      0.542        5.85\n# ℹ 25,559 more rows",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#pivot-reshape",
    "href": "duckdb-dplyr.html#pivot-reshape",
    "title": "DuckDB + dplyr (R)",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\n\n# library(tidyr) ## already loaded\n\nq3 |&gt;\n  pivot_longer(tip_amount:fare_amount) |&gt;\n  collect()\n\n# A tibble: 51,138 × 4\n# Groups:   passenger_count [18]\n   passenger_count trip_distance name       value\n             &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1               1           2.2 tip_amount 1.01 \n 2               1          16.8 tip_amount 5.58 \n 3               1           2.7 tip_amount 1.16 \n 4               1           5.9 tip_amount 1.93 \n 5               1           8.4 tip_amount 2.93 \n 6               0           2.4 tip_amount 0.924\n 7               1           3.8 tip_amount 1.46 \n 8               1           5.4 tip_amount 1.83 \n 9               1           9.3 tip_amount 3.40 \n10               1           9.8 tip_amount 3.60 \n# ℹ 51,128 more rows",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#joins-merges",
    "href": "duckdb-dplyr.html#joins-merges",
    "title": "DuckDB + dplyr (R)",
    "section": "Joins (merges)",
    "text": "Joins (merges)\n\nmean_tips  = nyc |&gt; summarise(mean_tips = mean(tip_amount), .by = month)\nmean_fares = nyc |&gt; summarise(mean_fares = mean(fare_amount), .by = month)\n\nAgain, these commands complete instantly because all computation has been deferred until absolutely necessary (i.e.,. lazy eval).\n\nleft_join(\n  mean_fares,\n  mean_tips\n  ) |&gt;\n  collect()\n\nJoining with `by = join_by(month)`\n\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n# A tibble: 12 × 3\n   month mean_fares mean_tips\n   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1    11      12.3       1.25\n 2     7      10.4       1.06\n 3     8      10.5       1.08\n 4     1       9.81      1.01\n 5     4      10.3       1.04\n 6     5      10.6       1.08\n 7     9      12.4       1.25\n 8    10      12.5       1.28\n 9     2       9.94      1.04\n10    12      12.3       1.24\n11     3      10.2       1.06\n12     6      10.5       1.09",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#windowing",
    "href": "duckdb-dplyr.html#windowing",
    "title": "DuckDB + dplyr (R)",
    "section": "Windowing",
    "text": "Windowing\nIf you recall from the native SQL API, we sampled 1 percent of the data before creating decile bins to reduce the computation burden of sorting the entire table. Unfortunately, this approach doesn’t work as well for the dplyr frontend because the underlying SQL translation uses a generic sampling approach (rather than DuckDB’s optimised USING SAMPLE statement.)",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#close-connection",
    "href": "duckdb-dplyr.html#close-connection",
    "title": "DuckDB + dplyr (R)",
    "section": "Close connection",
    "text": "Close connection\n\ndbDisconnect(con)",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#appendix-related-interfaces",
    "href": "duckdb-dplyr.html#appendix-related-interfaces",
    "title": "DuckDB + dplyr (R)",
    "section": "Appendix: Related interfaces",
    "text": "Appendix: Related interfaces\n\narrow+duckdb\n\nlibrary(arrow)\nlibrary(duckdb) ## Already loaded\nlibrary(dplyr)  ## Already loaded\nlibrary(tidyr)  ## Already loaded\n\nWhen going through the arrow intermediary, we don’t need to establish a database with DBI::dbConnect like we did above. Instead, we can create a link (pointers) to the dataset on disk directly via the arrow::open_dataset() convience function. Here I’ll assign it to a new R object called nyc2.\n\nnyc2 = open_dataset(\"nyc-taxi\")\n\n\n\n\n\n\n\nopen_dataset() versus read_parquet()\n\n\n\n(For individual parquet files, we could just read then via arrow::read_parquet(), perhaps efficiently subsetting columns at the same time. But I find the open_dataset is generally what I’m looking for.)\n\n\nNote that printing our nyc2 dataset to the R console will just display the data schema. This is a cheap and convenient way to quickly interrogate the basic structure of your data, including column types, etc.\n\nnyc2\n\nFileSystemDataset with 12 Parquet files\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\npickup_location_id: int64\ndropoff_location_id: int64\nyear: int32\nmonth: int32\n\n\nThe key step for this “arrow + duckdb” dplyr workflow is to pass our arrow dataset to DuckDB via the to_duckdb() function.\n\nto_duckdb(nyc2)\n\n# Source:   table&lt;arrow_001&gt; [?? x 24]\n# Database: DuckDB v0.10.1 [root@Darwin 23.4.0:R 4.4.0/:memory:]\n   vendor_name pickup_datetime     dropoff_datetime    passenger_count\n   &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;                        &lt;dbl&gt;\n 1 CMT         2012-01-20 14:09:36 2012-01-20 14:42:25               1\n 2 CMT         2012-01-20 14:54:10 2012-01-20 15:06:55               1\n 3 CMT         2012-01-20 08:08:01 2012-01-20 08:11:02               1\n 4 CMT         2012-01-20 08:36:22 2012-01-20 08:39:44               1\n 5 CMT         2012-01-20 20:58:32 2012-01-20 21:03:04               1\n 6 CMT         2012-01-20 19:40:20 2012-01-20 19:43:43               2\n 7 CMT         2012-01-21 01:54:37 2012-01-21 02:08:02               2\n 8 CMT         2012-01-21 01:55:47 2012-01-21 02:08:51               3\n 9 VTS         2012-01-07 22:20:00 2012-01-07 22:27:00               2\n10 VTS         2012-01-10 07:11:00 2012-01-10 07:21:00               1\n# ℹ more rows\n# ℹ 20 more variables: trip_distance &lt;dbl&gt;, pickup_longitude &lt;dbl&gt;,\n#   pickup_latitude &lt;dbl&gt;, rate_code &lt;chr&gt;, store_and_fwd &lt;chr&gt;,\n#   dropoff_longitude &lt;dbl&gt;, dropoff_latitude &lt;dbl&gt;, payment_type &lt;chr&gt;,\n#   fare_amount &lt;dbl&gt;, extra &lt;dbl&gt;, mta_tax &lt;dbl&gt;, tip_amount &lt;dbl&gt;,\n#   tolls_amount &lt;dbl&gt;, total_amount &lt;dbl&gt;, improvement_surcharge &lt;dbl&gt;,\n#   congestion_surcharge &lt;dbl&gt;, pickup_location_id &lt;dbl&gt;, …\n\n\nNote that this transfer from Arrow to DuckDB is very quick (and memory cheap) because it is a zero copy. We are just passing around pointers instead of actually moving any data. See this blog post for more details, but the high-level take away is that we are benefitting from the tightly integrated architectures of these two libraries.1\nAt this, point all of the regular dplyr workflow logic from above should carry over. Just remember to first pass the arrow dataset via the to_duckdb() funciton. For example, here’s our initial aggregation query again:\n\nnyc2 |&gt;\n  to_duckdb() |&gt; ## &lt;= key step\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  ) |&gt;\n  collect()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n# A tibble: 18 × 2\n   passenger_count mean_tip\n             &lt;dbl&gt;    &lt;dbl&gt;\n 1               5    1.10 \n 2               9    0.807\n 3              65    0    \n 4             177    1    \n 5               7    0.544\n 6               8    0.351\n 7              66    1.5  \n 8               1    1.15 \n 9             247    2.3  \n10               4    0.845\n11               0    0.862\n12             254    0    \n13             249    0    \n14               2    1.08 \n15             208    0    \n16              10    0    \n17               3    0.963\n18               6    1.13 \n\n\n\n\n\n\n\n\nArrow’s native acero engine\n\n\n\nSome of you may be used to performing computation with the arrow package without going through DuckDB. What’s happening here is that arrow provides its own computation engine called “acero”. This Arrow-native engine is actually pretty performant… albeit not a fast as DuckDB, nor as feature rich. So I personally recommend always passing to DuckDB if you can. Still, if you’re curious then you can test yourself by re-trying the code chunk, but commenting out the to_duckdb() line. For more details, see here.\n\n\n\n\nduckplyr\nThe new kid on the block is duckplyr (announcement / homepage). Without going into too much depth, the promise of duckplyr is that it can provide a “fully native” dplyr experience that is directly coupled to DuckDB’s query engine. So, for example, it won’t have to rely on DBI’s generic’ SQL translations. Instead, the relevant dplyr “verbs” are being directly translated to DuckDB’s relational API to construct logical query plans. If that’s too much jargon, just know that it should involve less overhead, fewer translation errors, and better optimization. Moreover, a goal of duckplyr is for it to be a drop-in replace for dplyr in general. In other words, you could just swap out library(dplyr) for library(duckplyr) and all of your data wrangling operations will come backed by the power of DuckDB. This includes for working on “regular” R data frames in memory.\nAll of this is exciting and I would urge you stay tuned. Right now, duckplyr is still marked as experimental and has a few rough edges. But the basics are there. For example:\n\nlibrary(duckplyr, warn.conflicts = FALSE)\n\nduckplyr_df_from_parquet(\"nyc-taxi/**/*.parquet\") |&gt;\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n\nmaterializing:\n---------------------\n--- Relation Tree ---\n---------------------\nAggregate [passenger_count, mean(tip_amount)]\n  read_parquet(nyc-taxi/**/*.parquet)\n\n---------------------\n-- Result Columns  --\n---------------------\n- passenger_count (BIGINT)\n- mean_tip (DOUBLE)\n\n   passenger_count  mean_tip\n1                2 1.0815798\n2              208 0.0000000\n3               10 0.0000000\n4                5 1.1027325\n5                9 0.8068000\n6               65 0.0000000\n7              177 1.0000000\n8                3 0.9629494\n9                6 1.1283649\n10               0 0.8620988\n11             249 0.0000000\n12             254 0.0000000\n13               8 0.3507692\n14               7 0.5441176\n15              66 1.5000000\n16               1 1.1510110\n17             247 2.3000000\n18               4 0.8445190",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "duckdb-dplyr.html#footnotes",
    "href": "duckdb-dplyr.html#footnotes",
    "title": "DuckDB + dplyr (R)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Similar” might be a better description than “integrated”, since DuckdB does not use the Arrow memory model. But they are both columnar-orientated (among other things) and so the end result is pretty seamless integration.↩︎",
    "crumbs": [
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "slides/slides.html#preliminaries",
    "href": "slides/slides.html#preliminaries",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Preliminaries",
    "text": "Preliminaries\nAgenda and expectations\nThese sparse slides are mostly intended to serve as a rough guide map.\n\nMost of what we’ll be doing is live coding and working through examples.\nI strongly encourage you try these examples on you own machines. Laptops are perfectly fine.\n\nNote: All of the material for today’s workshop are available on my website:\n\nhttps://grantmcdermott.com/duckdb-polars",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#preliminaries-1",
    "href": "slides/slides.html#preliminaries-1",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Preliminaries",
    "text": "Preliminaries\nRequirements\nImportant: Before continuing, please make sure that you have completed the requirements listed on the workshop website.\n\nInstall the required R and/or Python libraries.\nDownload some NYC taxi data.\n\nThe data download step can take 15-20 minutes, depending on your internet connection.",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#problem-statement",
    "href": "slides/slides.html#problem-statement",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Problem statement",
    "text": "Problem statement\nWhy this workshop?\nIt’s a trope, but “big data” is everywhere. This is true whether you work in tech (like I do now), or in academic research (like I used to).\nOTOH many of datasets that I find myself working with aren’t at the scale of truly huge data that might warrant a Spark cluster.\n\nWe’re talking anywhere between 100 MB to 50 GB. (Max a few billion rows; often in the millions or less.)\nCan I do my work without the pain of going through Spark?\n\nAnother factor is working in polyglot teams. It would be great to repurpose similar syntax and libraries across languages…",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#taster",
    "href": "slides/slides.html#taster",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Taster",
    "text": "Taster\nDuckDB example\n\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\nnyc = open_dataset(here::here(\"taxi-data\"))\nprettyNum(nrow(nyc), \",\")\n\n[1] \"178,544,324\"\n\ntic = Sys.time()\n\nnyc_summ = nyc |&gt;\n  to_duckdb() |&gt;\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  ) |&gt; \n  collect()\n\n(toc = Sys.time() - tic)\n\nTime difference of 0.8269699 secs",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#taster-1",
    "href": "slides/slides.html#taster-1",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Taster",
    "text": "Taster\nDuckDB example (cont.)\nWe just read a ~180 million row dataset (from disk!) and did a group-by aggregation on it.\nIn &lt; 1 second.\nOn a laptop.\n🤯\n\n Let’s do a quick horesrace comparison (similar grouped aggregation, but on a slightly smaller dataset)…",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#simple-benchmark-computation-time-only",
    "href": "slides/slides.html#simple-benchmark-computation-time-only",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Simple benchmark: Computation time only",
    "text": "Simple benchmark: Computation time only\nDuckDB and Polars are already plenty fast…",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#simple-benchmark-computation-time-data-io",
    "href": "slides/slides.html#simple-benchmark-computation-time-data-io",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Simple benchmark: Computation time + data I/O",
    "text": "Simple benchmark: Computation time + data I/O\n… but are even more impressive once we account for data import times",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#wait.-how",
    "href": "slides/slides.html#wait.-how",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Wait. How??",
    "text": "Wait. How??\nBetter disk storage 🤝 Better memory representation\nTwo coinciding (r)evolutions enable faster, smarter computation:\n\n\n1. Better on-disk storage\n\nMainly talking about the Parquet file format here.\nColumnar storage format allows better compression (much smaller footprint) and efficient random access to selected rows or columns (don’t have to read the whole dataset a la CSVs).\n\n\n2. Better in-memory representation\n\nStandardisation around the Apache Arrow format + columnar representation. (Allows zero copy, fewer cache misses, etc.)\nOLAP + deferred materialisation. (Rather than “eagerly” executing each query step, we can be “lazy” and optimise queries before executing them.)",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#scaling-up",
    "href": "slides/slides.html#scaling-up",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Scaling up",
    "text": "Scaling up\nEven moar benchmarks\nQuestion: Do these benchmarks hold and scale more generally? Answer: Yes. See Database-like ops benchmark.\nMoreover—and I think this is key—these kinds of benchmarks normally exclude the data I/O component… and the associated benefits of not having to hold the whole dataset in RAM.\n\nThere are some fantastically fast in-memory data wrangling libraries out there. (My personal faves: data.table and collapse.) But “in-memory” means that you always have to keep the full dataset in, well, memory. And this can be expensive.\nLibraries like DuckDB and Polars sidestep this problem, effectively supercharging your computer’s data wrangling powers.",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#examples",
    "href": "slides/slides.html#examples",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Examples",
    "text": "Examples\nLive coding sessions\nLet’s head back to the website to work through some notebooks.\nDuckDB\n\nDuckDB SQL\nDuckDB + dplyr (R)\nDuckDB + Ibis (Python)\n\nPolars\n\nPolars from R and Python",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#what-didnt-we-cover",
    "href": "slides/slides.html#what-didnt-we-cover",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "What didn’t we cover?",
    "text": "What didn’t we cover?\nOther cool features\n\nS3 I/O\n\nDuckDB & Polars can both read/write directly from/to S3. You just need to provision your AWS credentials. [Ex. 1, 2, 3]\nNote: I prefer/recommend the workflow we practiced today—first download to local disk via aws cli—to avoid network + I/O latency.\n\nGeospatial\n\nIMO the next iteration of geospatial computation will be built on top of the tools we’ve seen today (and related libs).\nDuckDB provides an excellent spatial extension (works with dplyr). See also the GeoParquet, GeoArrow, & GeoPolars initiatives.",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#what-didnt-we-cover-1",
    "href": "slides/slides.html#what-didnt-we-cover-1",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "What didn’t we cover?",
    "text": "What didn’t we cover?\nOther cool features (cont.)\n\nStreaming\n\nStreaming is the feature that enables working with bigger-than-RAM data.\nVery easy to use and/or adjust our workflow to these cases…\nDuckDB: Simply specify a disk-backed database when you first fire up your connection from Python or R, e.g.\n\ncon = dbConnect(duckdb(), dbdir = \"nyc.dbb\")\n\nPolars: Simply specify streaming when collecting, e.g.\n\nsome_query.collect(streaming=True)",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#what-didnt-we-cover-2",
    "href": "slides/slides.html#what-didnt-we-cover-2",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "What didn’t we cover?",
    "text": "What didn’t we cover?\nOther cool features (cont.)\n\nModeling\n\nThe modeling part of this workflow is less tightly integrated b/c we generally have to bring the data into RAM.\nBut being able to quickly I/O parts of large datasets makes it very easy to iteratively run analyses on subsets of your data. E.g., I typically pair with fixest for unmatched performance on high-dimensional data.\nYou can also run bespoke models via UDFs and/or predictions on database backends. [Ex. 1, 2, 3]\nFWIW I believe that the underlying matrix and linear algebra libraries for direct modeling with these tools are coming. [Ex. 1, 2]",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "slides/slides.html#resources",
    "href": "slides/slides.html#resources",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "Resources",
    "text": "Resources\nLearning more\nDuckDB\n\nDuckDB homepage. Includes a very informative blog and standalone documentation for the client APIs (Python, R, and many others).\nAlso check out Harlequin for a cool, shell-based DuckDB IDE.\n\nPolars\n\nPolars GitHub Repo. Contains links to the standalone documentation for the client APIS (Python, R, etc.)\nSide-by-side code comparisons (versus pandas, dplyr, etc.) are available in Modern Polars (in Python) and Codebook for Polars in R.",
    "crumbs": [
      "(Pretty) big data wrangling with DuckDB and Polars"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "",
    "text": "This workshop will introduce you to DuckDB and Polars, two data wrangling libraries at the frontier of high-performance computation. (See benchmarks.) In addition to being extremely fast and portable, both DuckDB and Polars provide user-friendly implementations across multiple languages. This makes them very well suited to production and applied research settings, without the overhead of tools like Spark. We will provide a variety of real-life examples in both R and Python, with the aim of getting participants up and running as quickly as possible. We will learn how wrangle datasets extending over several hundred million observations in a matter of seconds or less, using only our laptops. And we will learn how to scale to even larger contexts where the data exceeds our computers’ RAM capacity. Finally, we will also discuss some complementary tools and how these can be integrated for an efficient end-to-end workflow (data I/O -&gt; wrangling -&gt; analysis).\n\n\n\n\n\n\nDisclaimer\n\n\n\nThe content for this workshop has been prepared, and is presented, in my personal capacity. Any opinions expressed herein are my own and are not necessarily shared by my employer. Please do not share any recorded material without the express permission of myself or the workshop organisers."
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "(Pretty) big data wrangling with DuckDB and Polars",
    "section": "",
    "text": "This workshop will introduce you to DuckDB and Polars, two data wrangling libraries at the frontier of high-performance computation. (See benchmarks.) In addition to being extremely fast and portable, both DuckDB and Polars provide user-friendly implementations across multiple languages. This makes them very well suited to production and applied research settings, without the overhead of tools like Spark. We will provide a variety of real-life examples in both R and Python, with the aim of getting participants up and running as quickly as possible. We will learn how wrangle datasets extending over several hundred million observations in a matter of seconds or less, using only our laptops. And we will learn how to scale to even larger contexts where the data exceeds our computers’ RAM capacity. Finally, we will also discuss some complementary tools and how these can be integrated for an efficient end-to-end workflow (data I/O -&gt; wrangling -&gt; analysis).\n\n\n\n\n\n\nDisclaimer\n\n\n\nThe content for this workshop has been prepared, and is presented, in my personal capacity. Any opinions expressed herein are my own and are not necessarily shared by my employer. Please do not share any recorded material without the express permission of myself or the workshop organisers."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "requirements.html",
    "href": "requirements.html",
    "title": "Requirements",
    "section": "",
    "text": "In order to follow along with the examples in this workshop, you’ll need to install some R and/or Python packages, as well as download a reasonably large dataset. Please make sure that you have completed all requirements before the workshop starts!"
  },
  {
    "objectID": "requirements.html#r-and-python-packages",
    "href": "requirements.html#r-and-python-packages",
    "title": "Requirements",
    "section": "R and Python Packages",
    "text": "R and Python Packages\nFor this workshop, you have the option of following along in either R, Python, or both. Ideally, I’d recommend both since one of my goals is to demonstrate the close equivalency in workflows across languages. But I’ll leave that to you.1\n\nRPython\n\n\nRun the following commands in your R console.\ninstall.packages(c(\"duckdb\", \"arrow\", \"dplyr\", \"tidyr\", \"duckplyr\"))\npolars (and therefore tidypolars) are not on CRAN so we install them from R-universe. Details here.\nSys.setenv(NOT_CRAN = \"true\")\ninstall.packages(c(\"polars\", \"tidypolars\"), repos = \"https://community.r-multiverse.org\")\nNote that you will need polars &gt;= 0.19.1 and tidypolars &gt;= 0.10.1.\n\n\n\n\n\n\nR package binaries for Linux\n\n\n\nAre you an R user on a Linux machine? If so, I strongly recommend that you configure your user profile to pull in pre-compiled R package binaries for your distro from PPM, rather than installing source packages from CRAN (and then having to compile them on your own machine). This will greatly reduce installation times and other potential install headaches. If you haven’t done this already, or don’t know what I’m talking, then the simplest thing to do is to let the excellent rspm package (link) figure it out for you. Bonus: It will also resolve system dependencies at the same time.\n# Run these two commands before installing any other packages\ninstall.packages(\"rspm\")\nrspm::enable()\nP.S. Once you have installed the rspm package, you can add the following line to your ~/.Rprofile file and it will automatically figure everything out for you whenever you start a new R session. See the rspm website for additional tips around integration with renv projects and so on.\nsuppressMessages(rspm::enable())\n\n\n\n\nFirst create and activate a Python virtual environment from your terminal. (Important: I’ll assume that you are in the current root of this repo.) The exact command varies by operating system.\n# MacOS / Linux\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# Windows\npy -m venv .venv\n.venv\\Scripts\\activate.bat\nThen install the Python packages that we will be using.\npython3 -m pip install duckdb polars pyarrow pandas matplotlib -U\npython3 -m pip install 'ibis-framework[duckdb,polars]' -U\n\n\n\n\n\n\nVS Code\n\n\n\nIf you are using VS Code, then there are a few tweaks to this Python setup. First up, once you’ve create your .venv virtual environment, then should see a pop-up message to the effect of:\nWe noticed a new environment has been created. Do you want to select it for the\nworkspace folder?\nSelect “Yes”, then choose your Python interpreter (ideally Python 3.9 or higher).\nOnce that’s done, you will also need to install the ipykernel and jupyter packages in addition to the packages that I mentioned above. Moreover, I recommend install packages from within VS Code using cell magics, i.e.\n```{py}\n%pip install ipykernel jupyter -U\n%pip install duckdb polars pyarrow pandas -U\n%pip install 'ibis-framework[duckdb,polars]' -U\n```"
  },
  {
    "objectID": "requirements.html#nyc-taxi-data",
    "href": "requirements.html#nyc-taxi-data",
    "title": "Requirements",
    "section": "NYC taxi data",
    "text": "NYC taxi data\nFor this workshop, we’ll make use of the infamous well-known New York City taxi data.\n\nWe’ll just be downloading a single year’s worth of data from 2012. But that will be enough to demonstrate the point and it’s of comparable size to the “typical” dataset that I work with.\nThe final dataset is ~8.5 GB compressed on disk and can take 10-20 minutes to download, depending on your internet connection.\n\nYou can download the dataset with the below terminal commands.\n\n\n\n\n\n\nNote\n\n\n\nYou will need the aws cli tool (install link) for these next commands to work. This should be a quick and simple install (you do not need a AWS account), but see further below for some alternative download options.\n\n\nmkdir -p nyc-taxi/year=2012\naws s3 cp s3://voltrondata-labs-datasets/nyc-taxi/year=2012 nyc-taxi/year=2012 --recursive --no-sign-request\nBesides being relatively chonky, there are two features of this dataset that we’ll come back to since they are key to our workflow:\n\nThe data are stored in .parquet file format.\nThese Parquet files are organised in so-called “Hive-style” partitions on disk.\n\n\nOther data options\n\nSmaller subsets of the data\nIf you’re pressed for time and/or disk space, feel free to only grab a subset of the data manually. But make sure that you preserve the Hive-style partitioning. Here’s a quick example of how to do it for the first two months.\nmkdir -p nyc-taxi/year=2012/month=1\nmkdir -p nyc-taxi/year=2012/month=2\naws s3 cp s3://voltrondata-labs-datasets/nyc-taxi/year=2012/month=1/ nyc-taxi/year=2012/month=1 --recursive --no-sign-request\naws s3 cp s3://voltrondata-labs-datasets/nyc-taxi/year=2012/month=2/ nyc-taxi/year=2012/month=2 --recursive --no-sign-request\n\n\nAlternative download options\nIf you don’t have the aws cli tool, or can’t install install it for some reason, then you can always download the dataset directly from R or Python using some of the packages that we installed above. For example:\nlibrary(arrow)\nlibrary(dplyr)\n\ndata_path = \"nyc-taxi/year=2012\" # Or set your own preferred path\n\nopen_dataset(\"s3://voltrondata-labs-datasets/nyc-taxi/year=2012\") |&gt;\n    write_dataset(data_path, partitioning = \"month\")\nBe forewarned that these alternative download approaches are going to be slower than the aws cli approach."
  },
  {
    "objectID": "requirements.html#footnotes",
    "href": "requirements.html#footnotes",
    "title": "Requirements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re unsure and just want to pick one, then I recommend R. It’s much easier to install and manage environments. Plus it’s also my preferred language, so you’re likely to get better support from me.↩︎"
  },
  {
    "objectID": "duckdb-ibis.html",
    "href": "duckdb-ibis.html",
    "title": "DuckDB + Ibis (Python)",
    "section": "",
    "text": "import ibis\nimport ibis.selectors as s\nfrom ibis import _\n# ibis.options.interactive = True # enforce eager execution of queries",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#load-libraries",
    "href": "duckdb-ibis.html#load-libraries",
    "title": "DuckDB + Ibis (Python)",
    "section": "",
    "text": "import ibis\nimport ibis.selectors as s\nfrom ibis import _\n# ibis.options.interactive = True # enforce eager execution of queries",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#connect-and-register",
    "href": "duckdb-ibis.html#connect-and-register",
    "title": "DuckDB + Ibis (Python)",
    "section": "Connect and register",
    "text": "Connect and register\n\n## Instantiate an in-memory DuckDB connection from Ibis\ncon = ibis.duckdb.connect()\n\n## Register our parquet dataset as a table called \"nyc\" in our connection\ncon.register(\"nyc-taxi/**/*.parquet\", \"nyc\")\n\nDatabaseTable: nyc\n  vendor_name           string\n  pickup_datetime       timestamp\n  dropoff_datetime      timestamp\n  passenger_count       int64\n  trip_distance         float64\n  pickup_longitude      float64\n  pickup_latitude       float64\n  rate_code             string\n  store_and_fwd         string\n  dropoff_longitude     float64\n  dropoff_latitude      float64\n  payment_type          string\n  fare_amount           float64\n  extra                 float64\n  mta_tax               float64\n  tip_amount            float64\n  tolls_amount          float64\n  total_amount          float64\n  improvement_surcharge float64\n  congestion_surcharge  float64\n  pickup_location_id    int64\n  dropoff_location_id   int64\n  month                 int64\n  year                  int64\n\n\n\nAside: Remember that you can create a persistent, disk-backed database by giving it an appropriate name/path. This also enables out-of-core computation for bigger than RAM data.\n# con = ibis.duckdb.connect(\"nyc.dbb\")\n# con.register(\"nyc-taxi/**/*.parquet\", \"nyc\")\n# etc.\nReference the table from Python. We’ll call this reference object nyc too for consistency, but you could call it whatever you want (e.g., you could call it nyc_ibis to avoid potential ambiguity with the “nyc” table in our actual DuckDB connection). Printing the object to screen will give us a lazy preview.\n\n# con.list_tables() # Optional: confirm that our table is available\n\nnyc = con.table(\"nyc\")\nnyc\n\nDatabaseTable: nyc\n  vendor_name           string\n  pickup_datetime       timestamp\n  dropoff_datetime      timestamp\n  passenger_count       int64\n  trip_distance         float64\n  pickup_longitude      float64\n  pickup_latitude       float64\n  rate_code             string\n  store_and_fwd         string\n  dropoff_longitude     float64\n  dropoff_latitude      float64\n  payment_type          string\n  fare_amount           float64\n  extra                 float64\n  mta_tax               float64\n  tip_amount            float64\n  tolls_amount          float64\n  total_amount          float64\n  improvement_surcharge float64\n  congestion_surcharge  float64\n  pickup_location_id    int64\n  dropoff_location_id   int64\n  month                 int64\n  year                  int64",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#first-example",
    "href": "duckdb-ibis.html#first-example",
    "title": "DuckDB + Ibis (Python)",
    "section": "First example",
    "text": "First example\n\nq1 = (\n  nyc\n  .group_by([\"passenger_count\"])\n  .agg(mean_tip = _.tip_amount.mean())\n)\n\nTo see the underlying SQL translation, use ibis.to_sql()\n\nibis.to_sql(q1)\n\nSELECT\n  t0.passenger_count,\n  AVG(t0.tip_amount) AS mean_tip\nFROM main.nyc AS t0\nGROUP BY\n  1\n\n\nTo actually execute the query and bring the result into Python, we can use the execute() method. By default this will coerce to a pandas DataFrame.\n\ndat1 = q1.execute()\ndat1\n\n\n\n\n\n\n\n\npassenger_count\nmean_tip\n\n\n\n\n0\n0\n0.862099\n\n\n1\n254\n0.000000\n\n\n2\n249\n0.000000\n\n\n3\n1\n1.151011\n\n\n4\n247\n2.300000\n\n\n5\n4\n0.844519\n\n\n6\n3\n0.962949\n\n\n7\n6\n1.128365\n\n\n8\n5\n1.102732\n\n\n9\n9\n0.806800\n\n\n10\n177\n1.000000\n\n\n11\n65\n0.000000\n\n\n12\n2\n1.081580\n\n\n13\n208\n0.000000\n\n\n14\n10\n0.000000\n\n\n15\n7\n0.544118\n\n\n16\n8\n0.350769\n\n\n17\n66\n1.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIbis conversion to polars\n\n\n\nThe q1.execute() method above is equivalent calling q1.to_pandas(). A q1.to_polars() equivalent has been added to the dev version of Ibis, but is not available with the latest offical release (8.0.0 at the time of writing).\n\n\n\nDigression: Interactive Ibis use and eager execution\nAt the very top of this document, I commented out the ibis.options.interactive option as part of my Ibis configuration. This was because I wanted to demonstrate the default deferred (i.e., lazy) behaviour of Ibis, which is just the same as d(b)plyr in R. If you are building data wrangling pipelines, or writing scripts with potentially complex queries, you probably want to preserve this deferred behaviour and avoid eager execution.\nHowever, there are times when you may want to default into eager execution. For example, if your dataset is of manageable size, or you are trying to iterate through different query operations… Or, you might just want to enable it so that you automatically get a nice print return object for your workshop materials. I’ll adopt the latter view, so that I can quickly demonstrate some Ibis syntax and results for the rest of this document.\n\nibis.options.interactive = True\n\nOkay, let’s speed through some of the same basic queries that we’ve already seen in the DuckDB SQL and R (dplyr) pages. I won’t bother to explain them in depth. Just consider them for demonstration purposes.",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#aggregation",
    "href": "duckdb-ibis.html#aggregation",
    "title": "DuckDB + Ibis (Python)",
    "section": "Aggregation",
    "text": "Aggregation\n\n(\n  nyc\n  .group_by([\"passenger_count\", \"trip_distance\"])\n  .aggregate(\n    mean_tip = _.tip_amount.mean(),\n    mean_fare = _.fare_amount.mean()\n    )\n)\n\n\n\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ passenger_count ┃ trip_distance ┃ mean_tip ┃ mean_fare ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int64           │ float64       │ float64  │ float64   │\n├─────────────────┼───────────────┼──────────┼───────────┤\n│               1 │          0.90 │ 0.578975 │  5.687596 │\n│               2 │          8.80 │ 2.669911 │ 24.652684 │\n│               1 │          3.50 │ 1.384175 │ 12.883433 │\n│               1 │          2.90 │ 1.223380 │ 11.352555 │\n│               1 │          0.77 │ 0.499988 │  5.271770 │\n│               1 │          1.90 │ 0.911617 │  8.669678 │\n│               1 │          4.00 │ 1.516746 │ 14.098944 │\n│               2 │          0.90 │ 0.495665 │  5.764356 │\n│               1 │          0.10 │ 0.526552 │  6.079754 │\n│               2 │          1.90 │ 0.783820 │  8.720520 │\n│               … │             … │        … │         … │\n└─────────────────┴───────────────┴──────────┴───────────┘\n\n\n\nNote that, even though we have enabled the interactive print mode, we still get lazy evalation if we assign a chain of query steps to an object (here: q3)…\n\nq3 = (\n  nyc\n  .group_by([\"passenger_count\", \"trip_distance\"])\n  .agg(\n    mean_tip = _.tip_amount.mean(),\n    mean_fare = _.fare_amount.mean()\n    )\n)\n\n… but printing the query to screen enforces computation.\n\nq3\n\n\n\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ passenger_count ┃ trip_distance ┃ mean_tip ┃ mean_fare ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int64           │ float64       │ float64  │ float64   │\n├─────────────────┼───────────────┼──────────┼───────────┤\n│               1 │           2.0 │ 0.942336 │  8.932499 │\n│               1 │          10.8 │ 3.808778 │ 29.238556 │\n│               1 │           3.0 │ 1.250686 │ 11.611495 │\n│               1 │           1.2 │ 0.688208 │  6.640748 │\n│               1 │           1.3 │ 0.720728 │  6.941542 │\n│               1 │           3.6 │ 1.404022 │ 13.127911 │\n│               1 │           4.1 │ 1.545880 │ 14.321923 │\n│               1 │           1.4 │ 0.753561 │  7.243111 │\n│               2 │           3.3 │ 1.171171 │ 12.518436 │\n│               2 │           2.0 │ 0.810508 │  8.988705 │\n│               … │             … │        … │         … │\n└─────────────────┴───────────────┴──────────┴───────────┘",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#pivot-reshape",
    "href": "duckdb-ibis.html#pivot-reshape",
    "title": "DuckDB + Ibis (Python)",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\n\n# now chain on pivoting (and enforce computation via printing)\n(\n  q3\n  .pivot_longer(s.r[\"mean_tip\":\"mean_fare\"])\n)\n\n\n\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ passenger_count ┃ trip_distance ┃ name      ┃ value    ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩\n│ int64           │ float64       │ string    │ float64  │\n├─────────────────┼───────────────┼───────────┼──────────┤\n│               2 │           0.7 │ mean_tip  │ 0.433840 │\n│               2 │           0.7 │ mean_fare │ 5.117718 │\n│               1 │           0.8 │ mean_tip  │ 0.534714 │\n│               1 │           0.8 │ mean_fare │ 5.360443 │\n│               1 │           0.7 │ mean_tip  │ 0.493234 │\n│               1 │           0.7 │ mean_fare │ 5.035307 │\n│               2 │           0.3 │ mean_tip  │ 0.310850 │\n│               2 │           0.3 │ mean_fare │ 4.192242 │\n│               3 │           0.8 │ mean_tip  │ 0.414165 │\n│               3 │           0.8 │ mean_fare │ 5.470059 │\n│               … │             … │ …         │        … │\n└─────────────────┴───────────────┴───────────┴──────────┘",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-ibis.html#joins-merges",
    "href": "duckdb-ibis.html#joins-merges",
    "title": "DuckDB + Ibis (Python)",
    "section": "Joins (merges)",
    "text": "Joins (merges)\n(As we did in the dplyr code, we’ll break this contrived join example into two steps)\n\nmean_tips = nyc.group_by(\"month\").agg(mean_tip = _.tip_amount.mean())\nmean_fares = nyc.group_by(\"month\").agg(mean_fare = _.fare_amount.mean())\n\n\n(\n  mean_tips\n  .left_join(mean_fares, \"month\")\n)\n\n┏━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ month ┃ mean_tip ┃ month_right ┃ mean_fare ┃\n┡━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int64 │ float64  │ int64       │ float64   │\n├───────┼──────────┼─────────────┼───────────┤\n│    11 │ 1.250903 │          11 │ 12.270138 │\n│    10 │ 1.281239 │          10 │ 12.501252 │\n│     2 │ 1.036874 │           2 │  9.942640 │\n│     4 │ 1.043167 │           4 │ 10.335490 │\n│     1 │ 1.007817 │           1 │  9.813488 │\n│     5 │ 1.078014 │           5 │ 10.585157 │\n│     9 │ 1.254601 │           9 │ 12.391198 │\n│     7 │ 1.059312 │           7 │ 10.379943 │\n│     8 │ 1.079521 │           8 │ 10.492650 │\n│    12 │ 1.237651 │          12 │ 12.313953 │\n│     … │        … │           … │         … │\n└───────┴──────────┴─────────────┴───────────┘",
    "crumbs": [
      "DuckDB + Ibis (Python)"
    ]
  },
  {
    "objectID": "duckdb-sql.html",
    "href": "duckdb-sql.html",
    "title": "DuckDB SQL",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(duckdb)\n\nLoading required package: DBI\n\n\n\n\n\nimport duckdb\nimport time # just for timing some queries",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#load-libraries",
    "href": "duckdb-sql.html#load-libraries",
    "title": "DuckDB SQL",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(duckdb)\n\nLoading required package: DBI\n\n\n\n\n\nimport duckdb\nimport time # just for timing some queries",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#create-a-database-connection",
    "href": "duckdb-sql.html#create-a-database-connection",
    "title": "DuckDB SQL",
    "section": "Create a database connection",
    "text": "Create a database connection\nThe first thing we need to do is instantiate a connection with an in-memory database.1\n\nRPython\n\n\n\ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n\n\n\ncon = duckdb.connect(database = ':memory:', read_only = False)\n\n\n\n\n\nDigression: In-memory versus on-disk\nThe fact that our connection lives “in memory” is a killer feature of DuckDB (one that it inherits from SQLite). We don’t need to connect to some complicated, existing database infrastructure to harness all of DuckDB’s power. Instead we can just spin up an ephemeral database that interacts directly with our R (or Python, or Julia, etc.) client.\nHowever, it’s worth noting that you can create a persistent, disk-backed database simply by providing a database file path argument as part of your connection, e.g.\n\nRPython\n\n\n## Uncomment and run the next line if you'd like to create a persistent,\n## disk-backed database instead.\n\n# con = dbConnect(duckdb(), dbdir = \"nyc.duck\")\n\n\n## Uncomment and run the next line if you'd like to create a persistent,\n## disk-backed database instead.\n\n# con = duckdb.connect(database = 'nyc.duck', read_only = False)\n\n\n\n(Note that the \".duck\" file extension above is optional. You could also use \".db\", \".dbb\", or whatever you want really.)\n\n\n\n\n\n\nBigger than RAM data?\n\n\n\nOne really important benefit of creating a persistent disk-backed database is that it enables out-of-core computation for bigger than RAM data. See here for more details and performance considerations (which are still great).",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#first-example",
    "href": "duckdb-sql.html#first-example",
    "title": "DuckDB SQL",
    "section": "First example",
    "text": "First example\nWe’ll start with a simple aggregation query to get situated. I’ll also use this example to highlight some general features of DuckDB SQL and the underlying query engine.\nOkay, first query. Let’s say we want to know: What is the average tip per passenger count? A typical SQL job for this question might look as follows:\nSELECT\n  passenger_count,\n  AVG(tip_amount) AS mean_tip\nFROM 'nyc-taxi/**/*.parquet'\nGROUP BY passenger_count\nORDER BY passenger_count\n(Where the last ORDER BY statement is optional. Note that ordering (i.e., sorting) is a potentially expensive operation but we’ll get back to that later.)\nThis is perfectly valid DuckDB SQL too. However, we can rewrite it with slightly nicer syntax thanks DuckDB’s “friendly SQL”. The key changes for this simple query are going to be: (1) the FROM statement comes first, and (2) we can use the GROUP BY ALL and ORDER BY ALL statements to avoid writing out the “passenger_count” grouping column multiple times.2\nFROM 'nyc-taxi/**/*.parquet'\nSELECT\n  passenger_count,\n  AVG(tip_amount) AS mean_tip\nGROUP BY ALL\nORDER BY ALL\n\n\n\n\n\n\nDuckDB’s “friendly SQL”\n\n\n\nOne of the under-appreciated (IMHO) features of DuckDB is that it supports many syntax enhancements over tradional SQL dialects, which they collectively dub “friendly SQL”. Together these syntax enhancements allow you to write much more ergonomic SQL queries that cut down on duplication and logical inconsistencies.\n\n\nTo run this operation from our R or Python client, simply pass the SQL query as a string to our connection. Let’s use this as a chance to save the result and time our query too.\n\nRPython\n\n\n\ntic = Sys.time()\ndat1 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    passenger_count,\n    AVG(tip_amount) AS mean_tip\n  GROUP BY ALL\n  ORDER BY ALL\n  \"\n)\ntoc = Sys.time()\n\ndat1\n\n   passenger_count  mean_tip\n1                0 0.8620988\n2                1 1.1510110\n3                2 1.0815798\n4                3 0.9629494\n5                4 0.8445190\n6                5 1.1027325\n7                6 1.1283649\n8                7 0.5441176\n9                8 0.3507692\n10               9 0.8068000\n11              10 0.0000000\n12              65 0.0000000\n13              66 1.5000000\n14             177 1.0000000\n15             208 0.0000000\n16             247 2.3000000\n17             249 0.0000000\n18             254 0.0000000\n\ntoc - tic\n\nTime difference of 1.211972 secs\n\n\n\n\n\ntic = time.time()\ndat1 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    GROUP BY ALL\n    ORDER BY ALL\n    '''\n    )\n)\ntoc = time.time()\n\ndat1\n\n┌─────────────────┬─────────────────────┐\n│ passenger_count │      mean_tip       │\n│      int64      │       double        │\n├─────────────────┼─────────────────────┤\n│               0 │  0.8620988141424404 │\n│               1 │  1.1510109615454076 │\n│               2 │  1.0815798424001326 │\n│               3 │  0.9629493657892962 │\n│               4 │  0.8445189789660359 │\n│               5 │   1.102732453261797 │\n│               6 │  1.1283649236954338 │\n│               7 │  0.5441176470588235 │\n│               8 │ 0.35076923076923083 │\n│               9 │  0.8068000000000001 │\n│              10 │                 0.0 │\n│              65 │                 0.0 │\n│              66 │                 1.5 │\n│             177 │                 1.0 │\n│             208 │                 0.0 │\n│             247 │                 2.3 │\n│             249 │                 0.0 │\n│             254 │                 0.0 │\n├─────────────────┴─────────────────────┤\n│ 18 rows                     2 columns │\n└───────────────────────────────────────┘\n\n# print(f\"Time difference of {toc - tic} seconds\")\n## Timing will be misleading for this rendered Quarto doc, since we're calling\n## Python from R (via the reticulate package).\n\nNote that we actually get a polars DataFrame as a return object. Click the callout box below to learn more.\n\n\n\n\n\n\nResult conversion in Python (click to expand)\n\n\n\n\n\nBy default, the con.query method that we are using here will return a polars DataFrame object that Python understands “natively” (i.e., has a print method for and so on). Behind the scenes, this duckdb to polars integration relies on the pyarrow library being available to our Python environment, which have already installed for this workshop.\nIt’s also possible return other types of Python objects. For example, you can use the .df() method to coerce to a pandas DataFrame instead, among several other formats like numpy arrays. (Details here.) Given the focus of this workshop, it won’t surprise you to hear that I’m going to stick with the default polars conversion.\n\n\n\n\n\n\nSo that only took 1.21 seconds in this rendered Quarto doc (and will likely be even faster when you try in an interactive session). To underscore just how crazy impressive this is, recall that this includes the time that it takes to read the data from disk. I can almost guarantee that the read + serialization time alone for traditional data wrangling workflows would take several minutes, and probably crash my laptop RAM. Don’t forget that our full dataset is nearly 200 million rows deep and 30 columns wide.\nAside: We clearly have a few outlier typos in our dataset. 254 passengers in a single taxi trip? I don’t think so. We’d probably want to filter these out with a WHERE statement if we were doing serious analysis, but I’m just going to leave them in for this tutorial.",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#aggregation",
    "href": "duckdb-sql.html#aggregation",
    "title": "DuckDB SQL",
    "section": "Aggregation",
    "text": "Aggregation\nLet’s try out some more aggregation queries. How about a slightly variation on a our first example query, where we (a) add “month” as a second grouping variable, and (b) subset to only the first three months of the year.\n\nRPython\n\n\n\ntic = Sys.time()\ndat2 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    month,\n    passenger_count,\n    AVG(tip_amount) AS mean_tip\n  WHERE month &lt;= 3\n  GROUP BY ALL\n  \"\n    )\ntoc = Sys.time()\n\nhead(dat2)\n\n  month passenger_count mean_tip\n1     1               1 1.036863\n2     2               1 1.068490\n3     3               1 1.089205\n4     2               8 0.500000\n5     1               8 0.000000\n6     1               7 0.000000\n\ntoc - tic\n\nTime difference of 0.373965 secs\n\n\n\n\n\ntic = time.time()\ndat2 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      month,\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    WHERE month &lt;= 3\n    GROUP BY ALL\n    '''\n  )\n)\ntoc = time.time()\n\ndat2\n\n┌───────┬─────────────────┬────────────────────┐\n│ month │ passenger_count │      mean_tip      │\n│ int64 │      int64      │       double       │\n├───────┼─────────────────┼────────────────────┤\n│     1 │               8 │                0.0 │\n│     2 │               8 │                0.5 │\n│     1 │               7 │                0.0 │\n│     1 │               3 │ 0.8752659692390742 │\n│     1 │               6 │ 1.0175694433120148 │\n│     2 │               3 │ 0.8948976704752726 │\n│     2 │               6 │ 1.0218459360559857 │\n│     3 │               6 │ 1.0515659390082825 │\n│     3 │               3 │ 0.9121818858010082 │\n│     1 │               1 │ 1.0368628142310818 │\n│     · │               · │          ·         │\n│     · │               · │          ·         │\n│     · │               · │          ·         │\n│     2 │               2 │ 0.9908003546925661 │\n│     3 │               2 │ 1.0096468528132252 │\n│     3 │             208 │                0.0 │\n│     1 │             208 │                0.0 │\n│     1 │               5 │  1.001198485873298 │\n│     2 │               5 │ 1.0157674270380461 │\n│     3 │               5 │ 1.0353911898062576 │\n│     1 │              65 │                0.0 │\n│     2 │               9 │                0.0 │\n│     1 │               9 │                0.0 │\n├───────┴─────────────────┴────────────────────┤\n│ 29 rows (20 shown)                 3 columns │\n└──────────────────────────────────────────────┘\n\n# print(f\"Time difference of {toc - tic} seconds\")\n## Timing will be misleading for this rendered Quarto doc, since we're calling\n## Python from R (via the reticulate package).\n\n\n\n\nNote that this query completed even faster than the first one, even though we added another grouping variable. Reason: Subsetting along our Hive-partitioned parquet dataset allows DuckDB to take shortcuts. We can see this directly by prepending an EXPLAIN statement to our query to reveal the optmized query plan.\n\nRPython\n\n\n\ndbGetQuery(\n  con,\n  \"\n  EXPLAIN\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      month,\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    WHERE month &lt;= 3\n    GROUP BY ALL\n  \"\n)\n\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             #0            │\n│             #1            │\n│          avg(#2)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│           month           │\n│      passenger_count      │\n│         tip_amount        │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│       PARQUET_SCAN        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│           month           │\n│      passenger_count      │\n│         tip_amount        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│ File Filters: (month &lt;= 3)│\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│        EC: 44907396       │\n└───────────────────────────┘                             \n\n\n\n\n\ncon.query(\n  '''\n  EXPLAIN\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      month,\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    WHERE month &lt;= 3\n    GROUP BY ALL\n  '''\n)\n\n┌───────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│  explain_key  │                                            explain_value                                             │\n│    varchar    │                                               varchar                                                │\n├───────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│ physical_plan │ ┌───────────────────────────┐\\n│       HASH_GROUP_BY       │\\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\\n│    …  │\n└───────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\n\ntl;dr DuckDB is able to exploit the month partition of our dataset, so subsetting means that it can avoid unecessary data ingestion. Similarly, it only reads in a select group of columns; that’s what the “PROJECTION” part of the plan denotes. If nothing else, the take-home message is that DuckDB only does what it needs to. Laziness as a virtue!\nHere’s a final aggregation example, this time including a high-dimensional grouping column (i.e., “trip_distance”) and some additional aggregations.\n\nRPython\n\n\n\ntic = Sys.time()\ndat3 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    passenger_count,\n    trip_distance,\n    AVG(tip_amount) AS mean_tip,\n    AVG(fare_amount) AS mean_fare\n  GROUP BY ALL\n\"\n)\ntoc = Sys.time()\n\nnrow(dat3)\n\n[1] 25569\n\nhead(dat3)\n\n  passenger_count trip_distance  mean_tip mean_fare\n1               1          3.80 1.4617821 13.624269\n2               1          2.70 1.1625732 10.811488\n3               1          2.20 1.0099496  9.471702\n4               1          4.70 1.6794478 15.728522\n5               2          1.86 0.8184262  8.482737\n6               2          2.70 0.9988106 10.910493\n\ntoc - tic\n\nTime difference of 2.870613 secs\n\n\n\n\n\ntic = time.time()\ndat3 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      trip_distance,\n      AVG(tip_amount) AS mean_tip,\n      AVG(fare_amount) AS mean_fare\n    GROUP BY ALL\n    '''\n    )\n)\ntoc = time.time()\n\nlen(dat3)\n\n25569\n\ndat3\n\n┌─────────────────┬───────────────┬─────────────────────┬────────────────────┐\n│ passenger_count │ trip_distance │      mean_tip       │     mean_fare      │\n│      int64      │    double     │       double        │       double       │\n├─────────────────┼───────────────┼─────────────────────┼────────────────────┤\n│               1 │           2.7 │    1.16257323977722 │ 10.811487937005044 │\n│               1 │           8.5 │  3.0037572144209994 │ 23.762548777891176 │\n│               2 │           2.7 │  0.9988105937071349 │ 10.910492532335036 │\n│               1 │           1.8 │   0.878870484195116 │  8.402046190257858 │\n│               1 │           0.2 │  0.3864478915075284 │  4.671192577145984 │\n│               1 │           5.9 │  1.9316437836686586 │   18.4181575160648 │\n│               1 │           9.8 │   3.602063841563186 │  26.75012573394927 │\n│               1 │           4.7 │   1.679447806444995 │ 15.728522049844514 │\n│               1 │           2.2 │   1.009949551490608 │   9.47170239542306 │\n│               1 │          0.93 │  0.5659718504077408 │ 5.8204754998691905 │\n│               · │            ·  │           ·         │          ·         │\n│               · │            ·  │           ·         │          ·         │\n│               · │            ·  │           ·         │          ·         │\n│               5 │           3.2 │   1.213120921023685 │ 12.059595218889795 │\n│               2 │          4.58 │  1.5289010022859153 │  15.32215579391595 │\n│               3 │          1.34 │  0.6439547586916009 │  6.937227684596117 │\n│               5 │          0.69 │  0.4423211070274535 │  4.844665893786588 │\n│               1 │         17.93 │   4.768273278630039 │ 48.079521940777724 │\n│               6 │          0.49 │ 0.36281714343475185 │    4.2202365308804 │\n│               6 │          1.53 │  0.7403351611114536 │  7.555714373729288 │\n│               1 │         18.06 │   4.738210227272728 │ 48.059339488636354 │\n│               5 │          13.6 │  3.7402173913043484 │  36.93664596273292 │\n│               1 │         12.75 │   3.638386937159822 │  34.57689262741217 │\n├─────────────────┴───────────────┴─────────────────────┴────────────────────┤\n│ ? rows (&gt;9999 rows, 20 shown)                                    4 columns │\n└────────────────────────────────────────────────────────────────────────────┘\n\n# print(f\"Time difference of {toc - tic} seconds\")\n## Timing will be misleading for this rendered Quarto doc, since we're calling\n## Python from R (via the reticulate package).",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#pivot-reshape",
    "href": "duckdb-sql.html#pivot-reshape",
    "title": "DuckDB SQL",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\nLet’s explore some pivot (reshape) examples, by building off the previous query.\n\nUNPIVOT: wide =&gt; long\nPIVOT: long =&gt; wide\n\nHere I’ll use a Common Table Expression (CTE) to define a temporary table tmp_table, before unpivoting—i.e., reshaping long—at the end.\n\nRPython\n\n\n\ndat_long = dbGetQuery(\n  con,\n  \"\n  WITH tmp_table AS (\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      trip_distance,\n      AVG(tip_amount) AS mean_tip,\n      AVG(fare_amount) AS mean_fare\n    GROUP BY ALL\n  )\n  UNPIVOT tmp_table\n  ON mean_tip, mean_fare\n  INTO\n    NAME variable\n    VALUE amount\n  \"\n)\n\nhead(dat_long)\n\n  passenger_count trip_distance  variable     amount\n1               1           1.1  mean_tip  0.6530588\n2               1           1.1 mean_fare  6.3333552\n3               2          18.0  mean_tip  3.7746336\n4               2          18.0 mean_fare 48.1438661\n5               1           1.2  mean_tip  0.6882080\n6               1           1.2 mean_fare  6.6407479\n\n\n\n\n\ndat_long = (\n  con.\n  query(\n    '''\n    WITH tmp_table AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        passenger_count,\n        trip_distance,\n        AVG(tip_amount) AS mean_tip,\n        AVG(fare_amount) AS mean_fare\n      GROUP BY ALL\n    )\n    UNPIVOT tmp_table\n    ON mean_tip, mean_fare\n    INTO\n      NAME variable\n      VALUE amount\n    '''\n  )\n)\n\ndat_long\n\n┌─────────────────┬───────────────┬───────────┬────────────────────┐\n│ passenger_count │ trip_distance │ variable  │       amount       │\n│      int64      │    double     │  varchar  │       double       │\n├─────────────────┼───────────────┼───────────┼────────────────────┤\n│               1 │           0.9 │ mean_tip  │  0.578974821777743 │\n│               1 │           0.9 │ mean_fare │  5.687595795787245 │\n│               1 │          11.1 │ mean_tip  │  3.839358215996518 │\n│               1 │          11.1 │ mean_fare │ 29.974972782697066 │\n│               1 │           4.0 │ mean_tip  │ 1.5167455372913883 │\n│               1 │           4.0 │ mean_fare │ 14.098943870659395 │\n│               1 │          17.3 │ mean_tip  │  5.590380869171258 │\n│               1 │          17.3 │ mean_fare │ 47.364542924635145 │\n│               1 │          0.71 │ mean_tip  │ 0.4766168470138103 │\n│               1 │          0.71 │ mean_fare │  5.104461268390179 │\n│               · │            ·  │    ·      │           ·        │\n│               · │            ·  │    ·      │           ·        │\n│               · │            ·  │    ·      │           ·        │\n│               5 │          12.7 │ mean_tip  │   3.07948347107438 │\n│               5 │          12.7 │ mean_fare │  33.64896694214877 │\n│               2 │         12.25 │ mean_tip  │ 3.1529553264604813 │\n│               2 │         12.25 │ mean_fare │  32.42886597938145 │\n│               6 │          9.22 │ mean_tip  │  3.173150684931507 │\n│               6 │          9.22 │ mean_fare │ 25.668340943683408 │\n│               6 │          9.34 │ mean_tip  │ 2.8852453987730065 │\n│               6 │          9.34 │ mean_fare │  25.80444785276073 │\n│               1 │          33.5 │ mean_tip  │  5.801387283236995 │\n│               1 │          33.5 │ mean_fare │  56.67167630057804 │\n├─────────────────┴───────────────┴───────────┴────────────────────┤\n│ ? rows (&gt;9999 rows, 20 shown)                          4 columns │\n└──────────────────────────────────────────────────────────────────┘\n\n\n\n\n\nAnother option would have been to create a new table in memory and then pivot over that, which segues nicely to…\n\nDigression: Create new tables\nCTEs are a very common, and often efficient, way to implement multi-table operations in SQL. But, for the record, we can create new tables in DuckDB’s memory cache pretty easily using the CREATE TABLE statement.\n\nRPython\n\n\nInstead of DBI::dbGetQuery, we must now use DBI::dbExecute.\n\ndbExecute(\n    con,\n    \"\n    CREATE TABLE taxi2 AS\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        passenger_count,\n        trip_distance,\n        AVG(tip_amount) AS mean_tip,\n        AVG(fare_amount) AS mean_fare\n      GROUP BY ALL\n    \"\n)\n\n[1] 25569\n\ndbListTables(con)\n\n[1] \"taxi2\"\n\n\nFWIW, you can always remove a table with dbRemoveTable().\n\n\nInstead of con.query(), we must now use con.execute().\n\ncon.execute(\n  '''\n  CREATE TABLE taxi2 AS\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      trip_distance,\n      AVG(tip_amount) AS mean_tip,\n      AVG(fare_amount) AS mean_fare\n    GROUP BY ALL\n  '''\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x1062d3cb0&gt;\n\n# https://stackoverflow.com/q/75727685\ncon.query(\n  '''\n  SELECT table_name, estimated_size AS nrows, column_count AS ncols\n  FROM duckdb_tables;\n  '''\n)\n\n┌────────────┬───────┬───────┐\n│ table_name │ nrows │ ncols │\n│  varchar   │ int64 │ int64 │\n├────────────┼───────┼───────┤\n│ taxi2      │ 25569 │     4 │\n└────────────┴───────┴───────┘\n\n\n\n\n\n\n\nBack to reshaping\nWith our new taxi2 table in hand, let’s redo the previous unpivot query directly on this new table. This makes the actual (un)pivot statement a bit clearer… and also separates out the execution time.\n\nRPython\n\n\n\ndbGetQuery(\n  con,\n  \"\n  UNPIVOT taxi2\n  ON mean_tip, mean_fare\n  INTO\n    NAME variable\n    VALUE amount\n  LIMIT 5\n  \"\n)\n\n  passenger_count trip_distance  variable    amount\n1               1           1.1  mean_tip 0.6530588\n2               1           1.1 mean_fare 6.3333552\n3               1           1.2  mean_tip 0.6882080\n4               1           1.2 mean_fare 6.6407479\n5               1           1.3  mean_tip 0.7207278\n\n\n\n\n\ncon.query(\n  '''\n  UNPIVOT taxi2\n  ON mean_tip, mean_fare\n  INTO\n    NAME variable\n    VALUE amount\n  LIMIT 5\n  '''\n)\n\n┌─────────────────┬───────────────┬───────────┬────────────────────┐\n│ passenger_count │ trip_distance │ variable  │       amount       │\n│      int64      │    double     │  varchar  │       double       │\n├─────────────────┼───────────────┼───────────┼────────────────────┤\n│               1 │           2.7 │ mean_tip  │   1.16257323977722 │\n│               1 │           2.7 │ mean_fare │  10.81148793700505 │\n│               0 │           9.1 │ mean_tip  │  2.399264497878359 │\n│               0 │           9.1 │ mean_fare │ 23.355728429985852 │\n│               1 │           1.8 │ mean_tip  │ 0.8788704841951157 │\n└─────────────────┴───────────────┴───────────┴────────────────────┘\n\n\n\n\n\n(Note how crazy fast pivoting in DuckDB actually is.)",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#joins-merges",
    "href": "duckdb-sql.html#joins-merges",
    "title": "DuckDB SQL",
    "section": "Joins (merges)",
    "text": "Joins (merges)\nIt’s a bit hard to demonstrate a join with only a single main table. But here is a contrived example, where we calculate the mean monthly tips and the mean monthly fares as separate sub-queries (CTEs), before joining them together by month.\n\nRPython\n\n\n\ndbGetQuery(\n  con,\n  \"\n  WITH \n    mean_tips AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        month,\n        AVG(tip_amount) AS mean_tip\n      GROUP BY month\n    ),\n    mean_fares AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        month,\n        AVG(fare_amount) AS mean_fare\n      GROUP BY month \n    )\n  FROM mean_tips\n  LEFT JOIN mean_fares\n  USING (month)\n  SELECT *\n  ORDER BY mean_tips.month\n  \"\n)\n\n   month mean_tip mean_fare\n1      1 1.007817  9.813488\n2      2 1.036874  9.942640\n3      3 1.056353 10.223107\n4      4 1.043167 10.335490\n5      5 1.078014 10.585157\n6      6 1.091082 10.548651\n7      7 1.059312 10.379943\n8      8 1.079521 10.492650\n9      9 1.254601 12.391198\n10    10 1.281239 12.501252\n11    11 1.250903 12.270138\n12    12 1.237651 12.313953\n\n\n\n\n\ncon.query(\n  '''\n  WITH \n    mean_tips AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        month,\n        AVG(tip_amount) AS mean_tip\n      GROUP BY month\n    ),\n    mean_fares AS (\n      FROM 'nyc-taxi/**/*.parquet'\n      SELECT\n        month,\n        AVG(fare_amount) AS mean_fare\n      GROUP BY month \n    )\n  FROM mean_tips\n  LEFT JOIN mean_fares\n  USING (month)\n  SELECT *\n  ORDER BY mean_tips.month\n  '''\n)\n\n┌───────┬────────────────────┬────────────────────┐\n│ month │      mean_tip      │     mean_fare      │\n│ int64 │       double       │       double       │\n├───────┼────────────────────┼────────────────────┤\n│     1 │ 1.0078165246989366 │  9.813487671828813 │\n│     2 │ 1.0368737381553987 │  9.942640301300228 │\n│     3 │   1.05635274287244 │ 10.223107216153554 │\n│     4 │ 1.0431674901411208 │ 10.335489610549338 │\n│     5 │ 1.0780143169836092 │ 10.585156844134143 │\n│     6 │  1.091082009381275 │ 10.548651231531922 │\n│     7 │  1.059312239456315 │ 10.379943069577804 │\n│     8 │ 1.0795208991227114 │ 10.492650001890153 │\n│     9 │ 1.2546008978994332 │ 12.391197540031698 │\n│    10 │ 1.2812392796882088 │ 12.501252484194163 │\n│    11 │ 1.2509031985269687 │ 12.270137514944446 │\n│    12 │ 1.2376507362291407 │ 12.313952857613234 │\n├───────┴────────────────────┴────────────────────┤\n│ 12 rows                               3 columns │\n└─────────────────────────────────────────────────┘\n\n\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nRedo the above join but, rather than using CTEs, use tables that you first create in DuckDB’s memory bank. Again, this will simplify the actual join operation and also emphasise how crazy fast joins are in DuckDB.",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#windowing",
    "href": "duckdb-sql.html#windowing",
    "title": "DuckDB SQL",
    "section": "Windowing",
    "text": "Windowing\nOne last example: Binning “trip_distance” into deciles and then calculating the the mean “tip_amount” within each decile. This is an example of a window function and query pattern that I use all the time in my own work. I find it extremely useful for quickly pulling out descriptive patterns from large datasets, from which I can then develop a better intuition of my data. In turn, this shapes the hypotheses and modeling choices that I make in the subsequent analysis stage.\n\n\n\n\n\n\nSorting and sampling\n\n\n\nI’m using a 1% random sample of my data here (see the USING SAMPLE 1% statement). Why? Because calculating deciles requires ranking your data and this is expensive! To rank data, we first have to sort it (ORDER) and this requires evaluating/comparing every single row in your dataset. In turn, this means that it’s very hard to take shortcuts. (This is one reason why DuckDB’s optimized query plan will always delay sorting until as late as possible; to only sort on a smaller subset/aggregation of the data if possible.) FWIW, DuckDB’s sorting algorithm is still crazy fast. But for data of this size, and where sorting on the full datset is unavoidable, I strongly recommend sampling first. Your general insights will almost certainly remain intact.\n\n\n\nRPython\n\n\n\ndbGetQuery(\n  con,\n  \"\n  WITH trip_deciles AS (\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      tip_amount,\n      trip_distance,\n      NTILE(10) OVER (ORDER BY trip_distance) AS decile\n    USING SAMPLE 1%\n  )\n  FROM trip_deciles\n  SELECT\n    decile,\n    AVG(trip_distance) AS mean_distance,\n    AVG(tip_amount) AS mean_tip\n  GROUP BY ALL\n  ORDER BY ALL\n  \"\n)\n\n   decile mean_distance  mean_tip\n1       1     0.4485001 0.5815993\n2       2     0.8077842 0.5140569\n3       3     1.0636530 0.5985021\n4       4     1.3244716 0.6870273\n5       5     1.6286108 0.7805147\n6       6     1.9959871 0.8992282\n7       7     2.4957121 1.0367072\n8       8     3.2551231 1.2510039\n9       9     4.7643520 1.6101116\n10     10    11.0504331 3.2807728\n\n\n\n\n\ncon.query(\n  '''\n  WITH trip_deciles AS (\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      tip_amount,\n      trip_distance,\n      NTILE(10) OVER (ORDER BY trip_distance) AS decile\n    USING SAMPLE 1%\n  )\n  FROM trip_deciles\n  SELECT\n    decile,\n    AVG(trip_distance) AS mean_distance,\n    AVG(tip_amount) AS mean_tip\n  GROUP BY ALL\n  ORDER BY ALL\n  '''\n)\n\n┌────────┬─────────────────────┬────────────────────┐\n│ decile │    mean_distance    │      mean_tip      │\n│ int64  │       double        │       double       │\n├────────┼─────────────────────┼────────────────────┤\n│      1 │ 0.44922310548259337 │ 0.5799620572865318 │\n│      2 │  0.8087209464724292 │ 0.5141380253494722 │\n│      3 │  1.0653553928716593 │ 0.6061122685320124 │\n│      4 │    1.32654500168768 │ 0.6880050396303666 │\n│      5 │  1.6308654663142164 │ 0.7849700881062599 │\n│      6 │  1.9978597283474906 │ 0.8918140923428113 │\n│      7 │    2.49924847821658 │  1.044309233114935 │\n│      8 │   3.258975663125616 │    1.2555553486423 │\n│      9 │   4.760698918160984 │   1.62306336819192 │\n│     10 │  11.029094432514512 │  3.272133126160228 │\n├────────┴─────────────────────┴────────────────────┤\n│ 10 rows                                 3 columns │\n└───────────────────────────────────────────────────┘",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#close-connection",
    "href": "duckdb-sql.html#close-connection",
    "title": "DuckDB SQL",
    "section": "Close connection",
    "text": "Close connection\n\nRPython\n\n\n\ndbDisconnect(con)\n\nAgain, this step isn’t strictly necessary since we instantiated our connection with the shutdown = TRUE argument. But it’s worth seeing in case you want to be explicit.\n\n\n\ncon.close()",
    "crumbs": [
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "duckdb-sql.html#footnotes",
    "href": "duckdb-sql.html#footnotes",
    "title": "DuckDB SQL",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAside: The shutdown = TRUE argument is a convenience feature that ensures our connection is automatically terminated when our R session ends (i.e., even if we forget to do it manually.) I’m not aware of a similar convenience argument for Python; please let me know if I am missing something.↩︎\nI’ll admit that the benefits don’t seem so great for this simple example. But trust me: they make a big difference once you start having lots of grouping columns and complex sub-queries.↩︎",
    "crumbs": [
      "DuckDB SQL"
    ]
  }
]